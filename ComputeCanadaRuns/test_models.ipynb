{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA_DIR = '../NICU_Data/DepthFrameFullPrec_prePT/'\n",
    "ROOT_TRAIN_VAL_DATA_DIR = 'data/current/'\n",
    "SAVED_MODELS_DIR = 'savedModels/vit_tiny/prePT/'\n",
    "BASE_MODEL_NAME = 'vit_tiny_synthetic_prePT'\n",
    "TRAIN_DIR = ROOT_TRAIN_VAL_DATA_DIR + 'train/'\n",
    "VAL_DIR = ROOT_TRAIN_VAL_DATA_DIR + 'val/'\n",
    "K_FOLDS = 5\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "CLASS_WEIGHT = None\n",
    "INITIAL_BIAS = None\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.9\n",
    "FINETUNING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noone_p1 = glob(ROOT_DATA_DIR + 'p1/noone/*.png')\n",
    "nurse_p1 = glob(ROOT_DATA_DIR + 'p1/nurse/*.png')\n",
    "noone_p2 = glob(ROOT_DATA_DIR + 'p2/noone/*.png')\n",
    "nurse_p2 = glob(ROOT_DATA_DIR + 'p2/nurse/*.png')\n",
    "noone_p5 = glob(ROOT_DATA_DIR + 'p5/noone/*.png')\n",
    "nurse_p5 = glob(ROOT_DATA_DIR + 'p5/nurse/*.png')\n",
    "noone_p6 = glob(ROOT_DATA_DIR + 'p6/noone/*.png')\n",
    "nurse_p6 = glob(ROOT_DATA_DIR + 'p6/nurse/*.png')\n",
    "noone_p8 = glob(ROOT_DATA_DIR + 'p8/noone/*.png')\n",
    "nurse_p8 = glob(ROOT_DATA_DIR + 'p8/nurse/*.png')\n",
    "noone_p9 = glob(ROOT_DATA_DIR + 'p9/noone/*.png')\n",
    "nurse_p9 = glob(ROOT_DATA_DIR + 'p9/nurse/*.png')\n",
    "noone_p10 = glob(ROOT_DATA_DIR + 'p10/noone/*.png')\n",
    "nurse_p10 = glob(ROOT_DATA_DIR + 'p10/nurse/*.png')\n",
    "noone_p11 = glob(ROOT_DATA_DIR + 'p11/noone/*.png')\n",
    "nurse_p11 = glob(ROOT_DATA_DIR + 'p11/nurse/*.png')\n",
    "noone_p13 = glob(ROOT_DATA_DIR + 'p13/noone/*.png')\n",
    "nurse_p13 = glob(ROOT_DATA_DIR + 'p13/nurse/*.png')\n",
    "noone_p14 = glob(ROOT_DATA_DIR + 'p14/noone/*.png')\n",
    "nurse_p14 = glob(ROOT_DATA_DIR + 'p14/nurse/*.png')\n",
    "noone_p15 = glob(ROOT_DATA_DIR + 'p15/noone/*.png')\n",
    "nurse_p15 = glob(ROOT_DATA_DIR + 'p15/nurse/*.png')\n",
    "noone_p16 = glob(ROOT_DATA_DIR + 'p16/noone/*.png')\n",
    "nurse_p16 = glob(ROOT_DATA_DIR + 'p16/nurse/*.png')\n",
    "noone_p17 = glob(ROOT_DATA_DIR + 'p17/noone/*.png')\n",
    "nurse_p17 = glob(ROOT_DATA_DIR + 'p17/nurse/*.png')\n",
    "noone_p21 = glob(ROOT_DATA_DIR + 'p21/noone/*.png')\n",
    "nurse_p21 = glob(ROOT_DATA_DIR + 'p21/nurse/*.png')\n",
    "noone_p22 = glob(ROOT_DATA_DIR + 'p22/noone/*.png')\n",
    "nurse_p22 = glob(ROOT_DATA_DIR + 'p22/nurse/*.png')\n",
    "noone_p23 = glob(ROOT_DATA_DIR + 'p23/noone/*.png')\n",
    "nurse_p23 = glob(ROOT_DATA_DIR + 'p23/nurse/*.png')\n",
    "noone_p24 = glob(ROOT_DATA_DIR + 'p24/noone/*.png')\n",
    "nurse_p24 = glob(ROOT_DATA_DIR + 'p24/nurse/*.png')\n",
    "noone_p25 = glob(ROOT_DATA_DIR + 'p25/noone/*.png')\n",
    "nurse_p25 = glob(ROOT_DATA_DIR + 'p25/nurse/*.png')\n",
    "noone_p26 = glob(ROOT_DATA_DIR + 'p26/noone/*.png')\n",
    "nurse_p26 = glob(ROOT_DATA_DIR + 'p26/nurse/*.png')\n",
    "noone_p27 = glob(ROOT_DATA_DIR + 'p27/noone/*.png')\n",
    "nurse_p27 = glob(ROOT_DATA_DIR + 'p27/nurse/*.png')\n",
    "noone_p28 = glob(ROOT_DATA_DIR + 'p28/noone/*.png')\n",
    "nurse_p28 = glob(ROOT_DATA_DIR + 'p28/nurse/*.png')\n",
    "noone_p29 = glob(ROOT_DATA_DIR + 'p29/noone/*.png')\n",
    "nurse_p29 = glob(ROOT_DATA_DIR + 'p29/nurse/*.png')\n",
    "noone_p30 = glob(ROOT_DATA_DIR + 'p30/noone/*.png')\n",
    "nurse_p30 = glob(ROOT_DATA_DIR + 'p30/nurse/*.png')\n",
    "noone_p31 = glob(ROOT_DATA_DIR + 'p31/noone/*.png')\n",
    "nurse_p31 = glob(ROOT_DATA_DIR + 'p31/nurse/*.png')\n",
    "noone_p32 = glob(ROOT_DATA_DIR + 'p32/noone/*.png')\n",
    "nurse_p32 = glob(ROOT_DATA_DIR + 'p32/nurse/*.png')\n",
    "noone_p33 = glob(ROOT_DATA_DIR + 'p33/noone/*.png')\n",
    "nurse_p33 = glob(ROOT_DATA_DIR + 'p33/nurse/*.png')\n",
    "noone_p34 = glob(ROOT_DATA_DIR + 'p34/noone/*.png')\n",
    "nurse_p34 = glob(ROOT_DATA_DIR + 'p34/nurse/*.png')\n",
    "nurse_p90 = glob(ROOT_DATA_DIR + 'p90/nurse/*.png')\n",
    "nurse_p91 = glob(ROOT_DATA_DIR + 'p91/nurse/*.png')\n",
    "nurse_p92 = glob(ROOT_DATA_DIR + 'p92/nurse/*.png')\n",
    "nurse_p93 = glob(ROOT_DATA_DIR + 'p93/nurse/*.png')\n",
    "nurse_p94 = glob(ROOT_DATA_DIR + 'p94/nurse/*.png')\n",
    "\n",
    "\n",
    "all_noone_list = [noone_p1, noone_p2, noone_p5, noone_p6, noone_p8, noone_p9, noone_p10, noone_p11, noone_p13, noone_p14, noone_p15, noone_p16, noone_p17, noone_p21, noone_p22, noone_p23, noone_p24, noone_p25, noone_p26, noone_p27, noone_p28, noone_p29, noone_p30, noone_p31, noone_p32, noone_p33, noone_p34, [], [], [], [], []]\n",
    "all_nurse_list = [nurse_p1, nurse_p2, nurse_p5, nurse_p6, nurse_p8, nurse_p9, nurse_p10, nurse_p11, nurse_p13, nurse_p14, nurse_p15, nurse_p16, nurse_p17, nurse_p21, nurse_p22, nurse_p23, nurse_p24, nurse_p25, nurse_p26, nurse_p27, nurse_p28, nurse_p29, nurse_p30, nurse_p31, nurse_p32, nurse_p33, nurse_p34, nurse_p90, nurse_p91, nurse_p92, nurse_p93, nurse_p94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepFiles(noone_train, nurse_train, noone_val, nurse_val):\n",
    "    # Clean up train and val folders\n",
    "    remFiles = glob(TRAIN_DIR + 'noone/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(TRAIN_DIR + 'nurse/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(VAL_DIR + 'noone/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(VAL_DIR + 'nurse/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Move files of current fold into their folders\n",
    "    for f in noone_train:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = TRAIN_DIR + 'noone/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in nurse_train:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = TRAIN_DIR + 'nurse/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in noone_val:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = VAL_DIR + 'noone/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in nurse_val:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = VAL_DIR + 'nurse/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "\n",
    "    noone_train = glob(TRAIN_DIR + 'noone/*.png')\n",
    "    nurse_train = glob(TRAIN_DIR + 'nurse/*.png')\n",
    "\n",
    "    noone_val = glob(VAL_DIR + 'noone/*.png')\n",
    "    nurse_val = glob(VAL_DIR + 'nurse/*.png')\n",
    "\n",
    "    return noone_train, nurse_train, noone_val, nurse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "        transforms.RandomRotation(180),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(), # also rescales\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "        # transforms.RandomRotation(180),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        # transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(), # also rescales\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(finetuning=True):\n",
    "    model = timm.create_model('vit_tiny_patch16_224', pretrained=True, in_chans=1)\n",
    "\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = not finetuning\n",
    "\n",
    "    new_head = nn.Linear(model.head.in_features, 2, )\n",
    "    new_head.weight.data.fill_(0)\n",
    "    new_head.bias.data.fill_(0)\n",
    "    model.head = new_head\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printCM(cm, labels):\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax=ax)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    # ax.xaxis.set_ticklabels(labels)\n",
    "    # ax.yaxis.set_ticklabels(labels)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalModel(model, validation_dataloader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(validation_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted = predicted.data.cpu().numpy()\n",
    "\n",
    "            labels = labels.data.cpu().numpy()\n",
    "\n",
    "            y_pred.extend(predicted)\n",
    "            y_true.extend(labels)\n",
    "\n",
    "    classes = ('noone', 'nurse')\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    # printCM(cf_matrix, labels)\n",
    "\n",
    "    TN = cf_matrix[0][0]\n",
    "    FP = cf_matrix[0][1]\n",
    "    FN = cf_matrix[1][0]\n",
    "    TP = cf_matrix[1][1]\n",
    "    prec = TP / (TP + FP)\n",
    "    spec = TN / (TN + FP)\n",
    "    sens = TP / (TP + FN)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    f1 = (2 * TP) / ((2 * TP) + FP + FN)\n",
    "    print(f'TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}')\n",
    "    print(f\"Sens: {sens}, Spec: {spec}, Prec: {prec}, Acc: {acc}, F1: {f1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "noone_train: 11195, nurse_train: 1667\n",
      "noone_val: 2437, nurse_val: 193\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run2_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:31<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 162, FP: 15, FN: 31, TN: 2422\n",
      "Sens: 0.8393782383419689, Spec: 0.9938448912597456, Prec: 0.9152542372881356, Acc: 0.9825095057034221, F1: 0.8756756756756757\n",
      "\n",
      "Fold: 1\n",
      "noone_train: 11076, nurse_train: 1544\n",
      "noone_val: 2556, nurse_val: 316\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run2_fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:34<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 280, FP: 80, FN: 36, TN: 2476\n",
      "Sens: 0.8860759493670886, Spec: 0.9687010954616588, Prec: 0.7777777777777778, Acc: 0.9596100278551533, F1: 0.8284023668639053\n",
      "\n",
      "Fold: 2\n",
      "noone_train: 11105, nurse_train: 1558\n",
      "noone_val: 2527, nurse_val: 302\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run2_fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177/177 [00:34<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 227, FP: 33, FN: 75, TN: 2494\n",
      "Sens: 0.7516556291390728, Spec: 0.9869410368025326, Prec: 0.8730769230769231, Acc: 0.9618239660657476, F1: 0.8078291814946619\n",
      "\n",
      "Fold: 3\n",
      "noone_train: 10368, nurse_train: 1655\n",
      "noone_val: 3264, nurse_val: 205\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run2_fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:42<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 129, FP: 10, FN: 76, TN: 3254\n",
      "Sens: 0.6292682926829268, Spec: 0.9969362745098039, Prec: 0.9280575539568345, Acc: 0.9752089939463823, F1: 0.75\n",
      "\n",
      "Fold: 4\n",
      "noone_train: 10784, nurse_train: 1616\n",
      "noone_val: 2848, nurse_val: 244\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run2_fold5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:38<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 202, FP: 23, FN: 42, TN: 2825\n",
      "Sens: 0.8278688524590164, Spec: 0.9919241573033708, Prec: 0.8977777777777778, Acc: 0.9789780077619664, F1: 0.8614072494669509\n",
      "\n",
      "Fold: 0\n",
      "noone_train: 11195, nurse_train: 1667\n",
      "noone_val: 2437, nurse_val: 193\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run3_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:30<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 164, FP: 48, FN: 29, TN: 2389\n",
      "Sens: 0.8497409326424871, Spec: 0.9803036520311859, Prec: 0.7735849056603774, Acc: 0.970722433460076, F1: 0.8098765432098766\n",
      "\n",
      "Fold: 1\n",
      "noone_train: 11076, nurse_train: 1544\n",
      "noone_val: 2556, nurse_val: 316\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run3_fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:34<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 263, FP: 44, FN: 53, TN: 2512\n",
      "Sens: 0.8322784810126582, Spec: 0.9827856025039123, Prec: 0.8566775244299675, Acc: 0.966225626740947, F1: 0.8443017656500803\n",
      "\n",
      "Fold: 2\n",
      "noone_train: 11105, nurse_train: 1558\n",
      "noone_val: 2527, nurse_val: 302\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run3_fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177/177 [00:34<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 242, FP: 62, FN: 60, TN: 2465\n",
      "Sens: 0.8013245033112583, Spec: 0.9754649782350613, Prec: 0.7960526315789473, Acc: 0.9568752209261223, F1: 0.7986798679867987\n",
      "\n",
      "Fold: 3\n",
      "noone_train: 10368, nurse_train: 1655\n",
      "noone_val: 3264, nurse_val: 205\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run3_fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:42<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 151, FP: 102, FN: 54, TN: 3162\n",
      "Sens: 0.7365853658536585, Spec: 0.96875, Prec: 0.5968379446640316, Acc: 0.9550302680887864, F1: 0.6593886462882096\n",
      "\n",
      "Fold: 4\n",
      "noone_train: 10784, nurse_train: 1616\n",
      "noone_val: 2848, nurse_val: 244\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run3_fold5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:38<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 217, FP: 31, FN: 27, TN: 2817\n",
      "Sens: 0.889344262295082, Spec: 0.9891151685393258, Prec: 0.875, Acc: 0.98124191461837, F1: 0.8821138211382114\n",
      "\n",
      "Fold: 0\n",
      "noone_train: 11195, nurse_train: 1667\n",
      "noone_val: 2437, nurse_val: 193\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run4_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:34<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 166, FP: 61, FN: 27, TN: 2376\n",
      "Sens: 0.8601036269430051, Spec: 0.9749692244562987, Prec: 0.7312775330396476, Acc: 0.9665399239543726, F1: 0.7904761904761904\n",
      "\n",
      "Fold: 1\n",
      "noone_train: 11076, nurse_train: 1544\n",
      "noone_val: 2556, nurse_val: 316\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run4_fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:37<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 221, FP: 26, FN: 95, TN: 2530\n",
      "Sens: 0.6993670886075949, Spec: 0.9898278560250391, Prec: 0.8947368421052632, Acc: 0.9578690807799443, F1: 0.7850799289520426\n",
      "\n",
      "Fold: 2\n",
      "noone_train: 11105, nurse_train: 1558\n",
      "noone_val: 2527, nurse_val: 302\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run4_fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177/177 [00:37<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 232, FP: 45, FN: 70, TN: 2482\n",
      "Sens: 0.7682119205298014, Spec: 0.9821923229125445, Prec: 0.8375451263537906, Acc: 0.959349593495935, F1: 0.8013816925734024\n",
      "\n",
      "Fold: 3\n",
      "noone_train: 10368, nurse_train: 1655\n",
      "noone_val: 3264, nurse_val: 205\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run4_fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:45<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 154, FP: 198, FN: 51, TN: 3066\n",
      "Sens: 0.751219512195122, Spec: 0.9393382352941176, Prec: 0.4375, Acc: 0.928221389449409, F1: 0.5529622980251346\n",
      "\n",
      "Fold: 4\n",
      "noone_train: 10784, nurse_train: 1616\n",
      "noone_val: 2848, nurse_val: 244\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run4_fold5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:41<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 210, FP: 27, FN: 34, TN: 2821\n",
      "Sens: 0.860655737704918, Spec: 0.9905196629213483, Prec: 0.8860759493670886, Acc: 0.9802716688227684, F1: 0.8731808731808732\n",
      "\n",
      "Fold: 0\n",
      "noone_train: 11195, nurse_train: 1667\n",
      "noone_val: 2437, nurse_val: 193\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run5_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:33<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 170, FP: 27, FN: 23, TN: 2410\n",
      "Sens: 0.8808290155440415, Spec: 0.988920804267542, Prec: 0.8629441624365483, Acc: 0.9809885931558935, F1: 0.8717948717948718\n",
      "\n",
      "Fold: 1\n",
      "noone_train: 11076, nurse_train: 1544\n",
      "noone_val: 2556, nurse_val: 316\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run5_fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:35<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 264, FP: 224, FN: 52, TN: 2332\n",
      "Sens: 0.8354430379746836, Spec: 0.9123630672926447, Prec: 0.5409836065573771, Acc: 0.903899721448468, F1: 0.6567164179104478\n",
      "\n",
      "Fold: 2\n",
      "noone_train: 11105, nurse_train: 1558\n",
      "noone_val: 2527, nurse_val: 302\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run5_fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177/177 [00:34<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 248, FP: 35, FN: 54, TN: 2492\n",
      "Sens: 0.8211920529801324, Spec: 0.9861495844875346, Prec: 0.8763250883392226, Acc: 0.9685401201838105, F1: 0.8478632478632478\n",
      "\n",
      "Fold: 3\n",
      "noone_train: 10368, nurse_train: 1655\n",
      "noone_val: 3264, nurse_val: 205\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run5_fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:42<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 143, FP: 49, FN: 62, TN: 3215\n",
      "Sens: 0.697560975609756, Spec: 0.9849877450980392, Prec: 0.7447916666666666, Acc: 0.968002306140098, F1: 0.7204030226700252\n",
      "\n",
      "Fold: 4\n",
      "noone_train: 10784, nurse_train: 1616\n",
      "noone_val: 2848, nurse_val: 244\n",
      "\n",
      "['noone', 'nurse']\n",
      "Loaded model from savedModels/vit_tiny/prePT/vit_tiny_synthetic_prePT_run5_fold5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:38<00:00,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 209, FP: 30, FN: 35, TN: 2818\n",
      "Sens: 0.8565573770491803, Spec: 0.9894662921348315, Prec: 0.8744769874476988, Acc: 0.9789780077619664, F1: 0.865424430641822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_train_idx = np.array([\n",
    "    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
    "    ])\n",
    "all_val_idx = (all_train_idx == 0).astype('int32')\n",
    "\n",
    "runNum = 2\n",
    "totalRuns = 4\n",
    "\n",
    "for currRun in range(int(totalRuns)):\n",
    "    foldNum = 0\n",
    "    runSaveNum = int(runNum) + int(currRun)\n",
    "    for train_index, val_index in zip(all_train_idx, all_val_idx):\n",
    "        train_idx = train_index.nonzero()[0]\n",
    "        val_idx = val_index.nonzero()[0]\n",
    "\n",
    "        print(f'Fold: {foldNum}')\n",
    "\n",
    "        # Select the current folds for training and validation\n",
    "        noone_train = []\n",
    "        nurse_train = []\n",
    "        noone_val = []\n",
    "        nurse_val = []\n",
    "        for i in train_idx:\n",
    "            noone_train += all_noone_list[i]\n",
    "            nurse_train += all_nurse_list[i]\n",
    "        for i in val_idx:\n",
    "            noone_val += all_noone_list[i]\n",
    "            nurse_val += all_nurse_list[i]\n",
    "\n",
    "        # Move images to training and testing folders\n",
    "        noone_train, nurse_train, noone_val, nurse_val = prepFiles(noone_train, nurse_train, noone_val, nurse_val)\n",
    "\n",
    "        print(f\"noone_train: {len(noone_train)}, nurse_train: {len(nurse_train)}\")\n",
    "        print(f\"noone_val: {len(noone_val)}, nurse_val: {len(nurse_val)}\\n\")\n",
    "\n",
    "        weight_for_0 = (1 / (len(noone_train) + len(noone_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "        weight_for_1 = (1 / (len(nurse_train) + len(nurse_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "        CLASS_WEIGHT = torch.tensor([weight_for_0, weight_for_1], device=device)\n",
    "\n",
    "        # import data\n",
    "        sets = ['train', 'val']\n",
    "        image_datasets = {x: datasets.ImageFolder(os.path.join(ROOT_TRAIN_VAL_DATA_DIR, x), data_transforms[x]) for x in sets}\n",
    "        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True) for x in sets}\n",
    "        dataset_sizes = {x: len(image_datasets[x]) for x in sets}\n",
    "\n",
    "        class_names = image_datasets['train'].classes\n",
    "        print(class_names)\n",
    "\n",
    "        ###\n",
    "        # train everything\n",
    "\n",
    "        model = create_model(finetuning=FINETUNING)\n",
    "\n",
    "        foldNum += 1\n",
    "\n",
    "        modelPath = SAVED_MODELS_DIR + BASE_MODEL_NAME + \"_run\" + str(runSaveNum) + \"_fold\" + str(foldNum) + \".pth\"\n",
    "\n",
    "        model.load_state_dict(torch.load(modelPath))\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        print(f'Loaded model from {modelPath}')\n",
    "\n",
    "        evalModel(model, dataloaders['val'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf8b9ac8caee8cd6ae392661a519e2d4fbbc720f8ec71fc2b548fd3ad16b33c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
