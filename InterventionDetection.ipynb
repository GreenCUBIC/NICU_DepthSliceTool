{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA_DIR = 'bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/'\n",
    "TRAIN_DIR = ROOT_DATA_DIR + 'train/'\n",
    "TEST_DIR = ROOT_DATA_DIR + 'test/'\n",
    "VAL_DIR = ROOT_DATA_DIR + 'val/'\n",
    "TEST_SIZE = 0\n",
    "K_FOLDS = 5\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "CLASS_WEIGHT = None\n",
    "INITIAL_BIAS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import libdst\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import shutil\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Rescaling, Concatenate, Conv2D, Softmax, ReLU, Input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if len(physical_devices) > 0:\n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#     print(\"GPU memory growth enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add p5, p17 back in later.\n",
    "\n",
    "noone_p1 = glob(ROOT_DATA_DIR + 'p1/noone/*.png')\n",
    "nurse_p1 = glob(ROOT_DATA_DIR + 'p1/nurse/*.png')\n",
    "noone_p2 = glob(ROOT_DATA_DIR + 'p2/noone/*.png')\n",
    "nurse_p2 = glob(ROOT_DATA_DIR + 'p2/nurse/*.png')\n",
    "# noone_p5 = glob(ROOT_DATA_DIR + 'p5/noone/*.png')\n",
    "# nurse_p5 = glob(ROOT_DATA_DIR + 'p5/nurse/*.png')\n",
    "noone_p6 = glob(ROOT_DATA_DIR + 'p6/noone/*.png')\n",
    "nurse_p6 = glob(ROOT_DATA_DIR + 'p6/nurse/*.png')\n",
    "noone_p8 = glob(ROOT_DATA_DIR + 'p8/noone/*.png')\n",
    "nurse_p8 = glob(ROOT_DATA_DIR + 'p8/nurse/*.png')\n",
    "noone_p9 = glob(ROOT_DATA_DIR + 'p9/noone/*.png')\n",
    "nurse_p9 = glob(ROOT_DATA_DIR + 'p9/nurse/*.png')\n",
    "noone_p10 = glob(ROOT_DATA_DIR + 'p10/noone/*.png')\n",
    "nurse_p10 = glob(ROOT_DATA_DIR + 'p10/nurse/*.png')\n",
    "noone_p11 = glob(ROOT_DATA_DIR + 'p11/noone/*.png')\n",
    "nurse_p11 = glob(ROOT_DATA_DIR + 'p11/nurse/*.png')\n",
    "noone_p13 = glob(ROOT_DATA_DIR + 'p13/noone/*.png')\n",
    "nurse_p13 = glob(ROOT_DATA_DIR + 'p13/nurse/*.png')\n",
    "noone_p14 = glob(ROOT_DATA_DIR + 'p14/noone/*.png')\n",
    "nurse_p14 = glob(ROOT_DATA_DIR + 'p14/nurse/*.png')\n",
    "noone_p15 = glob(ROOT_DATA_DIR + 'p15/noone/*.png')\n",
    "nurse_p15 = glob(ROOT_DATA_DIR + 'p15/nurse/*.png')\n",
    "noone_p16 = glob(ROOT_DATA_DIR + 'p16/noone/*.png')\n",
    "nurse_p16 = glob(ROOT_DATA_DIR + 'p16/nurse/*.png')\n",
    "# noone_p17 = glob(ROOT_DATA_DIR + 'p17/noone/*.png')\n",
    "# nurse_p17 = glob(ROOT_DATA_DIR + 'p17/nurse/*.png')\n",
    "noone_p18 = glob(ROOT_DATA_DIR + 'p18/noone/*.png')\n",
    "nurse_p18 = glob(ROOT_DATA_DIR + 'p18/nurse/*.png')\n",
    "noone_p19 = glob(ROOT_DATA_DIR + 'p19/noone/*.png')\n",
    "nurse_p19 = glob(ROOT_DATA_DIR + 'p19/nurse/*.png')\n",
    "noone_p21 = glob(ROOT_DATA_DIR + 'p21/noone/*.png')\n",
    "nurse_p21 = glob(ROOT_DATA_DIR + 'p21/nurse/*.png')\n",
    "noone_p22 = glob(ROOT_DATA_DIR + 'p22/noone/*.png')\n",
    "nurse_p22 = glob(ROOT_DATA_DIR + 'p22/nurse/*.png')\n",
    "# noone_p23 = glob(ROOT_DATA_DIR + 'p23/noone/*.png')\n",
    "# nurse_p23 = glob(ROOT_DATA_DIR + 'p23/nurse/*.png')\n",
    "noone_p24 = glob(ROOT_DATA_DIR + 'p24/noone/*.png')\n",
    "nurse_p24 = glob(ROOT_DATA_DIR + 'p24/nurse/*.png')\n",
    "# noone_p25 = glob(ROOT_DATA_DIR + 'p25/noone/*.png')\n",
    "# nurse_p25 = glob(ROOT_DATA_DIR + 'p25/nurse/*.png')\n",
    "noone_p26 = glob(ROOT_DATA_DIR + 'p26/noone/*.png')\n",
    "nurse_p26 = glob(ROOT_DATA_DIR + 'p26/nurse/*.png')\n",
    "noone_p27 = glob(ROOT_DATA_DIR + 'p27/noone/*.png')\n",
    "nurse_p27 = glob(ROOT_DATA_DIR + 'p27/nurse/*.png')\n",
    "noone_p28 = glob(ROOT_DATA_DIR + 'p28/noone/*.png')\n",
    "nurse_p28 = glob(ROOT_DATA_DIR + 'p28/nurse/*.png')\n",
    "noone_p29 = glob(ROOT_DATA_DIR + 'p29/noone/*.png')\n",
    "nurse_p29 = glob(ROOT_DATA_DIR + 'p29/nurse/*.png')\n",
    "noone_p30 = glob(ROOT_DATA_DIR + 'p30/noone/*.png')\n",
    "nurse_p30 = glob(ROOT_DATA_DIR + 'p30/nurse/*.png')\n",
    "noone_p31 = glob(ROOT_DATA_DIR + 'p31/noone/*.png')\n",
    "nurse_p31 = glob(ROOT_DATA_DIR + 'p31/nurse/*.png')\n",
    "noone_p32 = glob(ROOT_DATA_DIR + 'p32/noone/*.png')\n",
    "nurse_p32 = glob(ROOT_DATA_DIR + 'p32/nurse/*.png')\n",
    "noone_p33 = glob(ROOT_DATA_DIR + 'p33/noone/*.png')\n",
    "nurse_p33 = glob(ROOT_DATA_DIR + 'p33/nurse/*.png')\n",
    "noone_p34 = glob(ROOT_DATA_DIR + 'p34/noone/*.png')\n",
    "nurse_p34 = glob(ROOT_DATA_DIR + 'p34/nurse/*.png')\n",
    "noone_p35 = glob(ROOT_DATA_DIR + 'p35/noone/*.png')\n",
    "nurse_p35 = glob(ROOT_DATA_DIR + 'p35/nurse/*.png')\n",
    "noone_p36 = glob(ROOT_DATA_DIR + 'p36/noone/*.png')\n",
    "nurse_p36 = glob(ROOT_DATA_DIR + 'p36/nurse/*.png')\n",
    "noone_p37 = glob(ROOT_DATA_DIR + 'p37/noone/*.png')\n",
    "nurse_p37 = glob(ROOT_DATA_DIR + 'p37/nurse/*.png')\n",
    "noone_p38 = glob(ROOT_DATA_DIR + 'p38/noone/*.png')\n",
    "nurse_p38 = glob(ROOT_DATA_DIR + 'p38/nurse/*.png')\n",
    "\n",
    "# all_noone = noone_p1 + noone_p2 + noone_p5 + noone_p6 + noone_p8 + noone_p9 + noone_p10 + noone_p11 + noone_p13 + noone_p14 + noone_p15 + noone_p16 + noone_p17 + noone_p18 + noone_p19 + noone_p21 + noone_p22 + noone_p23 + noone_p24 + noone_p25 + noone_p26 + noone_p27 + noone_p28 + noone_p29 + noone_p30 + noone_p31 + noone_p32 + noone_p33 + noone_p34 + noone_p35 + noone_p36 + noone_p37 + noone_p38\n",
    "# all_nurse = nurse_p1 + nurse_p2 + nurse_p5 + nurse_p6 + nurse_p8 + nurse_p9 + nurse_p10 + nurse_p11 + nurse_p13 + nurse_p14 + nurse_p15 + nurse_p16 + nurse_p17 + nurse_p18 + nurse_p19 + nurse_p21 + nurse_p22 + nurse_p23 + nurse_p24 + nurse_p25 + nurse_p26 + nurse_p27 + nurse_p28 + nurse_p29 + nurse_p30 + nurse_p31 + nurse_p32 + nurse_p33 + nurse_p34 + nurse_p35 + nurse_p36 + nurse_p37 + nurse_p38\n",
    "\n",
    "all_noone_list = [noone_p1, noone_p2, noone_p6, noone_p8, noone_p9, noone_p10, noone_p11, noone_p13, noone_p14, noone_p15, noone_p16, noone_p18, noone_p19, noone_p21, noone_p22, noone_p24, noone_p26, noone_p27, noone_p28, noone_p29, noone_p30, noone_p31, noone_p32, noone_p33, noone_p34, noone_p35, noone_p36, noone_p37, noone_p38]\n",
    "all_nurse_list = [nurse_p1, nurse_p2, nurse_p6, nurse_p8, nurse_p9, nurse_p10, nurse_p11, nurse_p13, nurse_p14, nurse_p15, nurse_p16, nurse_p18, nurse_p19, nurse_p21, nurse_p22, nurse_p24, nurse_p26, nurse_p27, nurse_p28, nurse_p29, nurse_p30, nurse_p31, nurse_p32, nurse_p33, nurse_p34, nurse_p35, nurse_p36, nurse_p37, nurse_p38]\n",
    "\n",
    "# all_noone_list = [noone_p1, noone_p2, noone_p5, noone_p6, noone_p8, noone_p9, noone_p10, noone_p11, noone_p13, noone_p14, noone_p15, noone_p16, noone_p17, noone_p18, noone_p19, noone_p21, noone_p22, noone_p23, noone_p24, noone_p25, noone_p26, noone_p27, noone_p28, noone_p29, noone_p30, noone_p31, noone_p32, noone_p33, noone_p34, noone_p35, noone_p36, noone_p37, noone_p38]\n",
    "# all_nurse_list = [nurse_p1, nurse_p2, nurse_p5, nurse_p6, nurse_p8, nurse_p9, nurse_p10, nurse_p11, nurse_p13, nurse_p14, nurse_p15, nurse_p16, nurse_p17, nurse_p18, nurse_p19, nurse_p21, nurse_p22, nurse_p23, nurse_p24, nurse_p25, nurse_p26, nurse_p27, nurse_p28, nurse_p29, nurse_p30, nurse_p31, nurse_p32, nurse_p33, nurse_p34, nurse_p35, nurse_p36, nurse_p37, nurse_p38]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_noone_train = []\n",
    "# all_noone_test = []\n",
    "# for p in all_noone_list:\n",
    "#     if len(p) > 0:\n",
    "#         train, test = train_test_split(p, test_size=TEST_SIZE)\n",
    "#         all_noone_train += [train]\n",
    "#         all_noone_test += test\n",
    "\n",
    "# all_nurse_train = []\n",
    "# all_nurse_test = []\n",
    "# for p in all_nurse_list:\n",
    "#     if len(p) > 0:\n",
    "#         train, test = train_test_split(p, test_size=TEST_SIZE)\n",
    "#         all_nurse_train += [train]\n",
    "#         all_nurse_test += test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_noone_train = []\n",
    "all_noone_test = []\n",
    "all_nurse_train = []\n",
    "all_nurse_test = []\n",
    "\n",
    "if TEST_SIZE > 0:\n",
    "    train, test = train_test_split(range(len(all_noone_list)), test_size=TEST_SIZE)\n",
    "    \n",
    "    for i in train:\n",
    "        all_noone_train += [all_noone_list[i]]\n",
    "        all_nurse_train += [all_nurse_list[i]]\n",
    "\n",
    "    for i in test:\n",
    "        all_noone_test += all_noone_list[i]\n",
    "        all_nurse_test += all_nurse_list[i]\n",
    "elif TEST_SIZE == 0:\n",
    "    all_noone_train = all_noone_list\n",
    "    all_nurse_train = all_nurse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remFiles = glob(TEST_DIR + 'noone/'+ '*.png')\n",
    "for f in remFiles:\n",
    "    os.remove(f)\n",
    "\n",
    "remFiles = glob(TEST_DIR + 'nurse/'+ '*.png')\n",
    "for f in remFiles:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in all_noone_test:\n",
    "    basename = os.path.basename(f)\n",
    "    dst_path = TEST_DIR + 'noone/' + basename\n",
    "    shutil.copy(f, dst_path)\n",
    "\n",
    "for f in all_nurse_test:\n",
    "    basename = os.path.basename(f)\n",
    "    dst_path = TEST_DIR + 'nurse/' + basename\n",
    "    shutil.copy(f, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noone_test = glob(TEST_DIR + 'noone/*.png')\n",
    "nurse_test = glob(TEST_DIR + 'nurse/*.png')\n",
    "all_test = (noone_test, nurse_test)\n",
    "labels = ['noone', 'nurse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/65535,\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1/65535,\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1/65535,\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_batch, y_batch = next(test_generator)\n",
    "\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# plt.style.use('dark_background')\n",
    "# for k, (img, lbl) in enumerate(zip(x_batch, y_batch)):\n",
    "#     plt.subplot(4, 8, k+1)\n",
    "#     plt.imshow((img + 1) / 2)\n",
    "#     plt.title(\"Class: {}\".format(labels[int(lbl)]))\n",
    "#     plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = VGG16(weights='imagenet', include_top=True)\n",
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.layers[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_layers = test_model.layers[2:-1]\n",
    "# trans_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.layers\n",
    "# w = test_model.layers[1].get_weights()[0][:, :, 2, :]\n",
    "# w = w.reshape(3, 3, 1, 64)\n",
    "# b = test_model.layers[1].get_weights()[1]\n",
    "# first_conv2d = Conv2D(64, kernel_size=3, padding='same', kernel_initializer=tf.keras.initializers.zeros(), use_bias=True, activation='relu')\n",
    "\n",
    "# model = tf.keras.Sequential(\n",
    "#     [\n",
    "#         tf.keras.Input(shape=(224, 224, 1),name='input'),\n",
    "#         first_conv2d,\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         tf.keras.layers.Dense(10, activation=\"relu\",use_bias=True,bias_initializer='zeros',name='dense1'),\n",
    "#         tf.keras.layers.Dense(1, activation=\"softmax\",name='output'),\n",
    "#     ]\n",
    "# )\n",
    "# model.layers[0].get_weights()[0].shape\n",
    "\n",
    "# model.layers[0].set_weights([w, b])\n",
    "# model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.layers[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "        # tf.keras.metrics.TruePositives(name='tp'),\n",
    "        # tf.keras.metrics.FalsePositives(name='fp'),\n",
    "        # tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "        # tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "        # tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='AUC'),\n",
    "        tf.keras.metrics.AUC(name='prc', curve='PR'),\n",
    "    ]\n",
    "\n",
    "def create_model(finetuning=True, metrics=METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    \n",
    "    base_model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "    first_conv2d_bias = base_model.layers[1].get_weights()[-1]\n",
    "    first_conv2d_weights = base_model.layers[1].get_weights()[0][:, :, 2, :].reshape(3, 3, 1, 64)\n",
    "    \n",
    "    custom_input = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1), name=\"1-Channel_input\")\n",
    "    first_conv2d = Conv2D(64, kernel_size=3, padding='same', name=\"First_Conv2D\")\n",
    "    transfer_layers = base_model.layers[2:-1]\n",
    "    predictions = Dense(2, activation=\"softmax\", name=\"prediction_layer\", bias_initializer=output_bias)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(custom_input)\n",
    "    model.add(first_conv2d)\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.layers[0].set_weights([first_conv2d_weights, first_conv2d_bias])\n",
    "\n",
    "    for layer in transfer_layers:\n",
    "        # print(layer)\n",
    "        model.add(layer)\n",
    "\n",
    "    model.add(predictions)\n",
    "\n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "        # Use this for \n",
    "        optimizer=tf.optimizers.SGD(0.00001, momentum=0.9),\n",
    "        # optimizer=tf.optimizers.SGD(),\n",
    "        metrics=[METRICS],\n",
    "        )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepFiles(noone_train, nurse_train, noone_val, nurse_val):\n",
    "    # Clean up train and val folders\n",
    "    remFiles = glob(TRAIN_DIR + 'noone/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(TRAIN_DIR + 'nurse/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(VAL_DIR + 'noone/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(VAL_DIR + 'nurse/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Move files of current fold into their folders\n",
    "    for f in noone_train:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = TRAIN_DIR + 'noone/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in nurse_train:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = TRAIN_DIR + 'nurse/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in noone_val:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = VAL_DIR + 'noone/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in nurse_val:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = VAL_DIR + 'nurse/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "\n",
    "    noone_train = glob(TRAIN_DIR + 'noone/*.png')\n",
    "    nurse_train = glob(TRAIN_DIR + 'nurse/*.png')\n",
    "\n",
    "    noone_val = glob(VAL_DIR + 'noone/*.png')\n",
    "    nurse_val = glob(VAL_DIR + 'nurse/*.png')\n",
    "\n",
    "    return noone_train, nurse_train, noone_val, nurse_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Using Yasmina's Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=K_FOLDS)\n",
    "cv_split = kf.split(range(len(all_noone_train)))\n",
    "foldNum = 0\n",
    "\n",
    "foldHistories = []\n",
    "# foldMetrics = []\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    for train_index, val_index in cv_split:\n",
    "        print(\"Fold {}: \\n\".format(foldNum))\n",
    "\n",
    "        # Select the current folds for training and validation\n",
    "        noone_train = []\n",
    "        nurse_train = []\n",
    "        noone_val = []\n",
    "        nurse_val = []\n",
    "        for i in train_index:\n",
    "            noone_train += all_noone_train[i]\n",
    "            nurse_train += all_nurse_train[i]\n",
    "        for i in val_index:\n",
    "            noone_val += all_noone_train[i]\n",
    "            nurse_val += all_nurse_train[i]\n",
    "\n",
    "        # Move images to training and testing folders\n",
    "        noone_train, nurse_train, noone_val, nurse_val = prepFiles(noone_train, nurse_train, noone_val, nurse_val)\n",
    "        all_train = [noone_train, nurse_train]\n",
    "        all_val = [noone_val, nurse_val]\n",
    "\n",
    "        print(\"noone_train: {}, nurse_train: {}\".format(len(noone_train), len(nurse_train)))\n",
    "        print(\"noone_val: {}, nurse_val: {}\\n\".format(len(noone_val), len(nurse_val)))\n",
    "\n",
    "        weight_for_0 = (1 / (len(noone_train) + len(noone_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "        weight_for_1 = (1 / (len(nurse_train) + len(nurse_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "        CLASS_WEIGHT = {0: weight_for_0, 1: weight_for_1}\n",
    "        print(\"Class Weights: {}\\n\".format(CLASS_WEIGHT))\n",
    "        \n",
    "        INITIAL_BIAS = np.log([(len(nurse_train) + len(nurse_val)) / (len(noone_train) + len(noone_val))])\n",
    "        print(\"Initial Bias: {}\\n\".format(INITIAL_BIAS))\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            TRAIN_DIR,\n",
    "            target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "            color_mode='grayscale',\n",
    "            class_mode='categorical',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            seed=123,\n",
    "        )\n",
    "        val_generator = val_datagen.flow_from_directory(\n",
    "            VAL_DIR,\n",
    "            target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "            color_mode='grayscale',\n",
    "            class_mode='categorical',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            seed=123,\n",
    "        )\n",
    "\n",
    "        # model = create_model(output_bias=INITIAL_BIAS)\n",
    "        model = create_model()\n",
    "        model.summary()\n",
    "        checkpoint_path = \"SavedModels/cp_fold_{}_gpu.ckpt\".format(foldNum)\n",
    "        checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_prc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "        earlyStopping = EarlyStopping(monitor='prc', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "\n",
    "        history = model.fit(\n",
    "            x=train_generator,\n",
    "            epochs=20,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=1,\n",
    "            callbacks=[checkpoint, earlyStopping],\n",
    "            # class_weight=CLASS_WEIGHT,\n",
    "        )\n",
    "\n",
    "        foldHistories.append(history)\n",
    "\n",
    "        # metrics = model.evaluate(test_generator, return_dict=True)\n",
    "        # for name, value in metrics.items():\n",
    "        #     print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "        # foldMetrics.append(metrics)\n",
    "\n",
    "        foldNum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: \n",
      "\n",
      "noone_train: 11025, nurse_train: 1063\n",
      "noone_val: 3255, nurse_val: 297\n",
      "\n",
      "Class Weights: {0: 0.5476190476190476, 1: 5.75}\n",
      "\n",
      "Initial Bias: [-2.35137526]\n",
      "\n",
      "Found 12088 images belonging to 2 classes.\n",
      "Found 3552 images belonging to 2 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "378/378 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9182 - prc: 0.9020\n",
      "Epoch 1: val_prc improved from inf to 0.82374, saving model to SavedModels\\cp_fold_0_gpu.ckpt\n",
      "378/378 [==============================] - 2610s 7s/step - loss: 0.2972 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9182 - prc: 0.9020 - val_loss: 0.4717 - val_accuracy: 0.8438 - val_precision: 0.8438 - val_recall: 0.8438 - val_AUC: 0.8359 - val_prc: 0.8237\n",
      "Epoch 2/20\n",
      "378/378 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9276 - prc: 0.9134\n",
      "Epoch 2: val_prc did not improve from 0.82374\n",
      "378/378 [==============================] - 2630s 7s/step - loss: 0.2932 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9276 - prc: 0.9134 - val_loss: 0.3697 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750 - val_AUC: 0.9141 - val_prc: 0.9132\n",
      "Epoch 3/20\n",
      "378/378 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9365 - prc: 0.9265\n",
      "Epoch 3: val_prc did not improve from 0.82374\n",
      "378/378 [==============================] - 2664s 7s/step - loss: 0.2876 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9365 - prc: 0.9265 - val_loss: 0.4445 - val_accuracy: 0.8438 - val_precision: 0.8438 - val_recall: 0.8438 - val_AUC: 0.9219 - val_prc: 0.9219\n",
      "Epoch 4/20\n",
      "378/378 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9423 - prc: 0.9345\n",
      "Epoch 4: val_prc did not improve from 0.82374\n",
      "378/378 [==============================] - 2682s 7s/step - loss: 0.2818 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9423 - prc: 0.9345 - val_loss: 0.1699 - val_accuracy: 0.9688 - val_precision: 0.9688 - val_recall: 0.9688 - val_AUC: 0.9492 - val_prc: 0.9357\n",
      "Epoch 5/20\n",
      "378/378 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9440 - prc: 0.9365\n",
      "Epoch 5: val_prc did not improve from 0.82374\n",
      "378/378 [==============================] - 3257s 9s/step - loss: 0.2777 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9440 - prc: 0.9365 - val_loss: 0.1907 - val_accuracy: 0.9688 - val_precision: 0.9688 - val_recall: 0.9688 - val_AUC: 0.9990 - val_prc: 0.9990\n",
      "Epoch 6/20\n",
      "378/378 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9491 - prc: 0.9435\n",
      "Epoch 6: val_prc did not improve from 0.82374\n",
      "378/378 [==============================] - 3298s 9s/step - loss: 0.2703 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9491 - prc: 0.9435 - val_loss: 0.4551 - val_accuracy: 0.8125 - val_precision: 0.8125 - val_recall: 0.8125 - val_AUC: 0.9170 - val_prc: 0.9216\n",
      "Epoch 7/20\n",
      "378/378 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9523 - prc: 0.9460\n",
      "Epoch 7: val_prc did not improve from 0.82374\n",
      "378/378 [==============================] - 3454s 9s/step - loss: 0.2640 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9523 - prc: 0.9460 - val_loss: 0.2631 - val_accuracy: 0.9062 - val_precision: 0.9062 - val_recall: 0.9062 - val_AUC: 0.9785 - val_prc: 0.9795\n",
      "Epoch 8/20\n",
      "378/378 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9542 - prc: 0.9480\n",
      "Epoch 8: val_prc did not improve from 0.82374\n",
      "378/378 [==============================] - 3464s 9s/step - loss: 0.2590 - accuracy: 0.9121 - precision: 0.9121 - recall: 0.9121 - AUC: 0.9542 - prc: 0.9480 - val_loss: 0.1744 - val_accuracy: 0.9688 - val_precision: 0.9688 - val_recall: 0.9688 - val_AUC: 0.9736 - val_prc: 0.9741\n",
      "Epoch 9/20\n",
      "378/378 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.9124 - precision: 0.9124 - recall: 0.9124 - AUC: 0.9586 - prc: 0.9547\n",
      "Epoch 9: val_prc did not improve from 0.82374\n",
      "378/378 [==============================] - 3687s 10s/step - loss: 0.2488 - accuracy: 0.9124 - precision: 0.9124 - recall: 0.9124 - AUC: 0.9586 - prc: 0.9547 - val_loss: 0.1223 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_AUC: 1.0000 - val_prc: 1.0000\n",
      "Epoch 9: early stopping\n",
      "Fold 1: \n",
      "\n",
      "noone_train: 10933, nurse_train: 979\n",
      "noone_val: 3347, nurse_val: 381\n",
      "\n",
      "Class Weights: {0: 0.5476190476190476, 1: 5.75}\n",
      "\n",
      "Initial Bias: [-2.35137526]\n",
      "\n",
      "Found 11912 images belonging to 2 classes.\n",
      "Found 3728 images belonging to 2 classes.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.3000 - accuracy: 0.9097 - precision: 0.9097 - recall: 0.9097 - AUC: 0.9247 - prc: 0.9069\n",
      "Epoch 1: val_prc improved from inf to 0.87136, saving model to SavedModels\\cp_fold_1_gpu.ckpt\n",
      "373/373 [==============================] - 2435s 7s/step - loss: 0.3000 - accuracy: 0.9097 - precision: 0.9097 - recall: 0.9097 - AUC: 0.9247 - prc: 0.9069 - val_loss: 0.3938 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750 - val_AUC: 0.8711 - val_prc: 0.8714\n",
      "Epoch 2/20\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9354 - prc: 0.9255\n",
      "Epoch 2: val_prc did not improve from 0.87136\n",
      "373/373 [==============================] - 3126s 8s/step - loss: 0.2790 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9354 - prc: 0.9255 - val_loss: 0.3733 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750 - val_AUC: 0.8867 - val_prc: 0.8790\n",
      "Epoch 3/20\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.2708 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9466 - prc: 0.9388\n",
      "Epoch 3: val_prc did not improve from 0.87136\n",
      "373/373 [==============================] - 3058s 8s/step - loss: 0.2708 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9466 - prc: 0.9388 - val_loss: 0.3521 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750 - val_AUC: 0.9287 - val_prc: 0.9300\n",
      "Epoch 4/20\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9494 - prc: 0.9417\n",
      "Epoch 4: val_prc did not improve from 0.87136\n",
      "373/373 [==============================] - 3416s 9s/step - loss: 0.2640 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9494 - prc: 0.9417 - val_loss: 0.1499 - val_accuracy: 0.9688 - val_precision: 0.9688 - val_recall: 0.9688 - val_AUC: 0.9668 - val_prc: 0.9661\n",
      "Epoch 5/20\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.2586 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9515 - prc: 0.9438\n",
      "Epoch 5: val_prc did not improve from 0.87136\n",
      "373/373 [==============================] - 3246s 9s/step - loss: 0.2586 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9515 - prc: 0.9438 - val_loss: 0.3499 - val_accuracy: 0.9062 - val_precision: 0.9062 - val_recall: 0.9062 - val_AUC: 0.9131 - val_prc: 0.9057\n",
      "Epoch 6/20\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9547 - prc: 0.9487\n",
      "Epoch 6: val_prc did not improve from 0.87136\n",
      "373/373 [==============================] - 3510s 9s/step - loss: 0.2531 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9547 - prc: 0.9487 - val_loss: 0.1140 - val_accuracy: 0.9688 - val_precision: 0.9688 - val_recall: 0.9688 - val_AUC: 0.9990 - val_prc: 0.9990\n",
      "Epoch 7/20\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9557 - prc: 0.9489\n",
      "Epoch 7: val_prc did not improve from 0.87136\n",
      "373/373 [==============================] - 3393s 9s/step - loss: 0.2496 - accuracy: 0.9178 - precision: 0.9178 - recall: 0.9178 - AUC: 0.9557 - prc: 0.9489 - val_loss: 0.1965 - val_accuracy: 0.9375 - val_precision: 0.9375 - val_recall: 0.9375 - val_AUC: 0.9766 - val_prc: 0.9773\n",
      "Epoch 8/20\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.9179 - precision: 0.9179 - recall: 0.9179 - AUC: 0.9563 - prc: 0.9499\n",
      "Epoch 8: val_prc improved from 0.87136 to 0.81219, saving model to SavedModels\\cp_fold_1_gpu.ckpt\n",
      "373/373 [==============================] - 3622s 10s/step - loss: 0.2473 - accuracy: 0.9179 - precision: 0.9179 - recall: 0.9179 - AUC: 0.9563 - prc: 0.9499 - val_loss: 0.4658 - val_accuracy: 0.8438 - val_precision: 0.8438 - val_recall: 0.8438 - val_AUC: 0.8672 - val_prc: 0.8122\n",
      "Epoch 9/20\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.9185 - precision: 0.9185 - recall: 0.9185 - AUC: 0.9609 - prc: 0.9552\n",
      "Epoch 9: val_prc did not improve from 0.81219\n",
      "373/373 [==============================] - 3487s 9s/step - loss: 0.2386 - accuracy: 0.9185 - precision: 0.9185 - recall: 0.9185 - AUC: 0.9609 - prc: 0.9552 - val_loss: 0.2000 - val_accuracy: 0.9375 - val_precision: 0.9375 - val_recall: 0.9375 - val_AUC: 0.9961 - val_prc: 0.9962\n",
      "Epoch 9: early stopping\n",
      "Fold 2: \n",
      "\n",
      "noone_train: 11501, nurse_train: 1092\n",
      "noone_val: 2779, nurse_val: 268\n",
      "\n",
      "Class Weights: {0: 0.5476190476190476, 1: 5.75}\n",
      "\n",
      "Initial Bias: [-2.35137526]\n",
      "\n",
      "Found 12593 images belonging to 2 classes.\n",
      "Found 3047 images belonging to 2 classes.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.2947 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9213 - prc: 0.9050\n",
      "Epoch 1: val_prc improved from inf to 0.90607, saving model to SavedModels\\cp_fold_2_gpu.ckpt\n",
      "394/394 [==============================] - 3056s 8s/step - loss: 0.2947 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9213 - prc: 0.9050 - val_loss: 0.4664 - val_accuracy: 0.8438 - val_precision: 0.8438 - val_recall: 0.8438 - val_AUC: 0.9023 - val_prc: 0.9061\n",
      "Epoch 2/20\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9330 - prc: 0.9208\n",
      "Epoch 2: val_prc did not improve from 0.90607\n",
      "394/394 [==============================] - 2705s 7s/step - loss: 0.2880 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9330 - prc: 0.9208 - val_loss: 0.1598 - val_accuracy: 0.9688 - val_precision: 0.9688 - val_recall: 0.9688 - val_AUC: 0.9902 - val_prc: 0.9906\n",
      "Epoch 3/20\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9407 - prc: 0.9330\n",
      "Epoch 3: val_prc did not improve from 0.90607\n",
      "394/394 [==============================] - 2694s 7s/step - loss: 0.2819 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9407 - prc: 0.9330 - val_loss: 0.2041 - val_accuracy: 0.9375 - val_precision: 0.9375 - val_recall: 0.9375 - val_AUC: 0.9902 - val_prc: 0.9906\n",
      "Epoch 4/20\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9444 - prc: 0.9373\n",
      "Epoch 4: val_prc did not improve from 0.90607\n",
      "394/394 [==============================] - 2791s 7s/step - loss: 0.2767 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9444 - prc: 0.9373 - val_loss: 0.3425 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750 - val_AUC: 0.9229 - val_prc: 0.9244\n",
      "Epoch 5/20\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9485 - prc: 0.9418\n",
      "Epoch 5: val_prc improved from 0.90607 to 0.83160, saving model to SavedModels\\cp_fold_2_gpu.ckpt\n",
      "394/394 [==============================] - 2738s 7s/step - loss: 0.2703 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9485 - prc: 0.9418 - val_loss: 0.4550 - val_accuracy: 0.8438 - val_precision: 0.8438 - val_recall: 0.8438 - val_AUC: 0.8711 - val_prc: 0.8316\n",
      "Epoch 6/20\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9520 - prc: 0.9458\n",
      "Epoch 6: val_prc did not improve from 0.83160\n",
      "394/394 [==============================] - 3081s 8s/step - loss: 0.2640 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9520 - prc: 0.9458 - val_loss: 0.2402 - val_accuracy: 0.9062 - val_precision: 0.9062 - val_recall: 0.9062 - val_AUC: 0.9775 - val_prc: 0.9789\n",
      "Epoch 7/20\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.2632 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9515 - prc: 0.9447\n",
      "Epoch 7: val_prc did not improve from 0.83160\n",
      "394/394 [==============================] - 2730s 7s/step - loss: 0.2632 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9515 - prc: 0.9447 - val_loss: 0.2875 - val_accuracy: 0.9062 - val_precision: 0.9062 - val_recall: 0.9062 - val_AUC: 0.9639 - val_prc: 0.9662\n",
      "Epoch 8/20\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9542 - prc: 0.9479\n",
      "Epoch 8: val_prc did not improve from 0.83160\n",
      "394/394 [==============================] - 2793s 7s/step - loss: 0.2577 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9542 - prc: 0.9479 - val_loss: 0.1690 - val_accuracy: 0.9688 - val_precision: 0.9688 - val_recall: 0.9688 - val_AUC: 0.9873 - val_prc: 0.9878\n",
      "Epoch 9/20\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9562 - prc: 0.9507\n",
      "Epoch 9: val_prc did not improve from 0.83160\n",
      "394/394 [==============================] - 2756s 7s/step - loss: 0.2536 - accuracy: 0.9133 - precision: 0.9133 - recall: 0.9133 - AUC: 0.9562 - prc: 0.9507 - val_loss: 0.1275 - val_accuracy: 0.9688 - val_precision: 0.9688 - val_recall: 0.9688 - val_AUC: 0.9971 - val_prc: 0.9971\n",
      "Epoch 9: early stopping\n",
      "Fold 3: \n",
      "\n",
      "noone_train: 11879, nurse_train: 1103\n",
      "noone_val: 2401, nurse_val: 257\n",
      "\n",
      "Class Weights: {0: 0.5476190476190476, 1: 5.75}\n",
      "\n",
      "Initial Bias: [-2.35137526]\n",
      "\n",
      "Found 12982 images belonging to 2 classes.\n",
      "Found 2658 images belonging to 2 classes.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.9072 - precision: 0.9072 - recall: 0.9072 - AUC: 0.9247 - prc: 0.9068\n",
      "Epoch 1: val_prc improved from inf to 0.73669, saving model to SavedModels\\cp_fold_3_gpu.ckpt\n",
      "406/406 [==============================] - 3619s 9s/step - loss: 0.3008 - accuracy: 0.9072 - precision: 0.9072 - recall: 0.9072 - AUC: 0.9247 - prc: 0.9068 - val_loss: 0.4022 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750 - val_AUC: 0.8154 - val_prc: 0.7367\n",
      "Epoch 2/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.9150 - precision: 0.9150 - recall: 0.9150 - AUC: 0.9465 - prc: 0.9387\n",
      "Epoch 2: val_prc did not improve from 0.73669\n",
      "406/406 [==============================] - 3425s 8s/step - loss: 0.2730 - accuracy: 0.9150 - precision: 0.9150 - recall: 0.9150 - AUC: 0.9465 - prc: 0.9387 - val_loss: 0.5142 - val_accuracy: 0.8438 - val_precision: 0.8438 - val_recall: 0.8438 - val_AUC: 0.8877 - val_prc: 0.8905\n",
      "Epoch 3/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.9150 - precision: 0.9150 - recall: 0.9150 - AUC: 0.9544 - prc: 0.9491\n",
      "Epoch 3: val_prc did not improve from 0.73669\n",
      "406/406 [==============================] - 3656s 9s/step - loss: 0.2585 - accuracy: 0.9150 - precision: 0.9150 - recall: 0.9150 - AUC: 0.9544 - prc: 0.9491 - val_loss: 0.1146 - val_accuracy: 0.9688 - val_precision: 0.9688 - val_recall: 0.9688 - val_AUC: 0.9990 - val_prc: 0.9990\n",
      "Epoch 4/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.9152 - precision: 0.9152 - recall: 0.9152 - AUC: 0.9599 - prc: 0.9557\n",
      "Epoch 4: val_prc did not improve from 0.73669\n",
      "406/406 [==============================] - 3508s 9s/step - loss: 0.2436 - accuracy: 0.9152 - precision: 0.9152 - recall: 0.9152 - AUC: 0.9599 - prc: 0.9557 - val_loss: 0.2599 - val_accuracy: 0.9062 - val_precision: 0.9062 - val_recall: 0.9062 - val_AUC: 0.9805 - val_prc: 0.9816\n",
      "Epoch 5/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.2350 - accuracy: 0.9185 - precision: 0.9185 - recall: 0.9185 - AUC: 0.9624 - prc: 0.9583\n",
      "Epoch 5: val_prc did not improve from 0.73669\n",
      "406/406 [==============================] - 3610s 9s/step - loss: 0.2350 - accuracy: 0.9185 - precision: 0.9185 - recall: 0.9185 - AUC: 0.9624 - prc: 0.9583 - val_loss: 0.3138 - val_accuracy: 0.9062 - val_precision: 0.9062 - val_recall: 0.9062 - val_AUC: 0.9414 - val_prc: 0.9192\n",
      "Epoch 6/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.2232 - accuracy: 0.9229 - precision: 0.9229 - recall: 0.9229 - AUC: 0.9659 - prc: 0.9634\n",
      "Epoch 6: val_prc did not improve from 0.73669\n",
      "406/406 [==============================] - 3452s 9s/step - loss: 0.2232 - accuracy: 0.9229 - precision: 0.9229 - recall: 0.9229 - AUC: 0.9659 - prc: 0.9634 - val_loss: 0.1933 - val_accuracy: 0.9375 - val_precision: 0.9375 - val_recall: 0.9375 - val_AUC: 0.9912 - val_prc: 0.9916\n",
      "Epoch 7/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.2245 - accuracy: 0.9254 - precision: 0.9254 - recall: 0.9254 - AUC: 0.9655 - prc: 0.9618\n",
      "Epoch 7: val_prc did not improve from 0.73669\n",
      "406/406 [==============================] - 3731s 9s/step - loss: 0.2245 - accuracy: 0.9254 - precision: 0.9254 - recall: 0.9254 - AUC: 0.9655 - prc: 0.9618 - val_loss: 0.2372 - val_accuracy: 0.9062 - val_precision: 0.9062 - val_recall: 0.9062 - val_AUC: 0.9736 - val_prc: 0.9754\n",
      "Epoch 8/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.2124 - accuracy: 0.9281 - precision: 0.9281 - recall: 0.9281 - AUC: 0.9688 - prc: 0.9660\n",
      "Epoch 8: val_prc did not improve from 0.73669\n",
      "406/406 [==============================] - 3760s 9s/step - loss: 0.2124 - accuracy: 0.9281 - precision: 0.9281 - recall: 0.9281 - AUC: 0.9688 - prc: 0.9660 - val_loss: 0.3977 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750 - val_AUC: 0.9258 - val_prc: 0.9266\n",
      "Epoch 9/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.2097 - accuracy: 0.9294 - precision: 0.9294 - recall: 0.9294 - AUC: 0.9697 - prc: 0.9676\n",
      "Epoch 9: val_prc did not improve from 0.73669\n",
      "406/406 [==============================] - 3881s 10s/step - loss: 0.2097 - accuracy: 0.9294 - precision: 0.9294 - recall: 0.9294 - AUC: 0.9697 - prc: 0.9676 - val_loss: 0.1905 - val_accuracy: 0.9375 - val_precision: 0.9375 - val_recall: 0.9375 - val_AUC: 0.9805 - val_prc: 0.9814\n",
      "Epoch 9: early stopping\n",
      "Fold 4: \n",
      "\n",
      "noone_train: 11782, nurse_train: 1203\n",
      "noone_val: 2498, nurse_val: 157\n",
      "\n",
      "Class Weights: {0: 0.5476190476190476, 1: 5.75}\n",
      "\n",
      "Initial Bias: [-2.35137526]\n",
      "\n",
      "Found 12985 images belonging to 2 classes.\n",
      "Found 2655 images belonging to 2 classes.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.3109 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9115 - prc: 0.8915\n",
      "Epoch 1: val_prc improved from inf to 0.99904, saving model to SavedModels\\cp_fold_4_gpu.ckpt\n",
      "406/406 [==============================] - 3764s 9s/step - loss: 0.3109 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9115 - prc: 0.8915 - val_loss: 0.1590 - val_accuracy: 0.9688 - val_precision: 0.9688 - val_recall: 0.9688 - val_AUC: 0.9990 - val_prc: 0.9990\n",
      "Epoch 2/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.3064 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9181 - prc: 0.9016\n",
      "Epoch 2: val_prc improved from 0.99904 to 0.92787, saving model to SavedModels\\cp_fold_4_gpu.ckpt\n",
      "406/406 [==============================] - 3964s 10s/step - loss: 0.3064 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9181 - prc: 0.9016 - val_loss: 0.4733 - val_accuracy: 0.8125 - val_precision: 0.8125 - val_recall: 0.8125 - val_AUC: 0.9219 - val_prc: 0.9279\n",
      "Epoch 3/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9298 - prc: 0.9175\n",
      "Epoch 3: val_prc did not improve from 0.92787\n",
      "406/406 [==============================] - 3941s 10s/step - loss: 0.3012 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9298 - prc: 0.9175 - val_loss: 0.2967 - val_accuracy: 0.9062 - val_precision: 0.9062 - val_recall: 0.9062 - val_AUC: 0.9443 - val_prc: 0.9283\n",
      "Epoch 4/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9376 - prc: 0.9280\n",
      "Epoch 4: val_prc did not improve from 0.92787\n",
      "406/406 [==============================] - 3901s 10s/step - loss: 0.2941 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9376 - prc: 0.9280 - val_loss: 0.0624 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_AUC: 1.0000 - val_prc: 1.0000\n",
      "Epoch 5/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9409 - prc: 0.9314\n",
      "Epoch 5: val_prc did not improve from 0.92787\n",
      "406/406 [==============================] - 3938s 10s/step - loss: 0.2883 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9409 - prc: 0.9314 - val_loss: 0.2035 - val_accuracy: 0.9375 - val_precision: 0.9375 - val_recall: 0.9375 - val_AUC: 0.9795 - val_prc: 0.9802\n",
      "Epoch 6/20\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9466 - prc: 0.9390\n",
      "Epoch 6: val_prc did not improve from 0.92787\n",
      "406/406 [==============================] - 3906s 10s/step - loss: 0.2791 - accuracy: 0.9074 - precision: 0.9074 - recall: 0.9074 - AUC: 0.9466 - prc: 0.9390 - val_loss: 0.2690 - val_accuracy: 0.9375 - val_precision: 0.9375 - val_recall: 0.9375 - val_AUC: 0.9414 - val_prc: 0.9404\n",
      "Epoch 7/20\n",
      "197/406 [=============>................] - ETA: 33:30 - loss: 0.2719 - accuracy: 0.9088 - precision: 0.9088 - recall: 0.9088 - AUC: 0.9496 - prc: 0.9426"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Zalamaan\\Documents\\Repos\\depthSliceTool\\InterventionDetection.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(checkpoint_path, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_prc\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m earlyStopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprc\u001b[39m\u001b[39m'\u001b[39m, min_delta\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     x\u001b[39m=\u001b[39;49mtrain_generator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_generator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[checkpoint, earlyStopping],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39m# class_weight=CLASS_WEIGHT,\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m foldHistories\u001b[39m.\u001b[39mappend(history)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# metrics = model.evaluate(test_generator, return_dict=True)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# for name, value in metrics.items():\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m#     print(f\"{name}: {value:.4f}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X26sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# foldMetrics.append(metrics)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=K_FOLDS)\n",
    "cv_split = kf.split(range(len(all_noone_train)))\n",
    "foldNum = 0\n",
    "\n",
    "foldHistories = []\n",
    "# foldMetrics = []\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    for train_index, val_index in cv_split:\n",
    "        print(\"Fold {}: \\n\".format(foldNum))\n",
    "\n",
    "        # Select the current folds for training and validation\n",
    "        noone_train = []\n",
    "        nurse_train = []\n",
    "        noone_val = []\n",
    "        nurse_val = []\n",
    "        for i in train_index:\n",
    "            noone_train += all_noone_train[i]\n",
    "            nurse_train += all_nurse_train[i]\n",
    "        for i in val_index:\n",
    "            noone_val += all_noone_train[i]\n",
    "            nurse_val += all_nurse_train[i]\n",
    "\n",
    "        # Move images to training and testing folders\n",
    "        noone_train, nurse_train, noone_val, nurse_val = prepFiles(noone_train, nurse_train, noone_val, nurse_val)\n",
    "        all_train = [noone_train, nurse_train]\n",
    "        all_val = [noone_val, nurse_val]\n",
    "\n",
    "        print(\"noone_train: {}, nurse_train: {}\".format(len(noone_train), len(nurse_train)))\n",
    "        print(\"noone_val: {}, nurse_val: {}\\n\".format(len(noone_val), len(nurse_val)))\n",
    "\n",
    "        weight_for_0 = (1 / (len(noone_train) + len(noone_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "        weight_for_1 = (1 / (len(nurse_train) + len(nurse_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "        CLASS_WEIGHT = {0: weight_for_0, 1: weight_for_1}\n",
    "        print(\"Class Weights: {}\\n\".format(CLASS_WEIGHT))\n",
    "        \n",
    "        INITIAL_BIAS = np.log([(len(nurse_train) + len(nurse_val)) / (len(noone_train) + len(noone_val))])\n",
    "        print(\"Initial Bias: {}\\n\".format(INITIAL_BIAS))\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            TRAIN_DIR,\n",
    "            target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "            color_mode='grayscale',\n",
    "            class_mode='categorical',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            seed=123,\n",
    "        )\n",
    "        val_generator = val_datagen.flow_from_directory(\n",
    "            VAL_DIR,\n",
    "            target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "            color_mode='grayscale',\n",
    "            class_mode='categorical',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            seed=123,\n",
    "        )\n",
    "\n",
    "        # model = create_model(output_bias=INITIAL_BIAS)\n",
    "        model = create_model()\n",
    "        model.summary()\n",
    "        checkpoint_path = \"SavedModels/cp_fold_{}_gpu.ckpt\".format(foldNum)\n",
    "        checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_prc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "        earlyStopping = EarlyStopping(monitor='prc', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "\n",
    "        history = model.fit(\n",
    "            x=train_generator,\n",
    "            epochs=20,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=1,\n",
    "            callbacks=[checkpoint, earlyStopping],\n",
    "            # class_weight=CLASS_WEIGHT,\n",
    "        )\n",
    "\n",
    "        foldHistories.append(history)\n",
    "\n",
    "        # metrics = model.evaluate(test_generator, return_dict=True)\n",
    "        # for name, value in metrics.items():\n",
    "        #     print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "        # foldMetrics.append(metrics)\n",
    "\n",
    "        foldNum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set includes 0 negatives and 0 positives\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noone_test_final = glob(TEST_DIR + 'noone/*.png')\n",
    "nurse_test_final = glob(TEST_DIR + 'nurse/*.png')\n",
    "print(\"Test set includes {} negatives and {} positives\".format(len(noone_test_final), len(nurse_test_final)))\n",
    "\n",
    "def printCM(cm, labels):\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax=ax)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticklabels(labels)\n",
    "    return\n",
    "\n",
    "def predictNurse(model, img):\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    preds = model.predict(x, verbose=0)\n",
    "    label = 'nurse' if preds[0][1]>preds[0][0] else 'noone'\n",
    "    return label\n",
    "\n",
    "def evalModel(model):\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for i in range(len(noone_test_final)):\n",
    "        im = load_img(noone_test_final[i], color_mode='grayscale', target_size=(IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        label = predictNurse(model, im)\n",
    "        preds.append(label)\n",
    "        truths.append('noone')\n",
    "\n",
    "    for i in range(len(nurse_test_final)):\n",
    "        im = load_img(nurse_test_final[i], color_mode='grayscale', target_size=(IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        label = predictNurse(model, im)\n",
    "        preds.append(label)\n",
    "        truths.append('nurse')\n",
    "\n",
    "    labels = ['noone', 'nurse']\n",
    "    cm = confusion_matrix(truths, preds)\n",
    "    printCM(cm, labels)\n",
    "\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TP = cm[1][1]\n",
    "    prec = TP / (TP + FP)\n",
    "    spec = TN / (TN + FP)\n",
    "    sens = TP / (TP + FN)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    f1 = (2 * TP) / ((2 * TP) + FP + FN)\n",
    "    print(\"Sens: {}, Spec: {}, Prec: {}, Acc: {}, F1: {}\".format(sens, spec, prec, acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p1/nurse\\patient_1_10.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p2/nurse\\patient_2_244.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p6/nurse\\Patient6_13.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p8/nurse\\patient8_1.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p9/nurse\\patient_9_161.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p10/nurse\\Patient10_102.png\n",
      "\n",
      "\n",
      "\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p11/nurse\\Patient11_298.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p13/nurse\\Patient_13_100.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p14/nurse\\Patient_14_part2_122.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p15/nurse\\Patient_15_2_10.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p16/nurse\\Patient_16-1_10.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p18/nurse\\Patient_18_2_105.png\n",
      "\n",
      "\n",
      "\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p19/nurse\\patient_19_12.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p21/nurse\\Patient_21_restart_308.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p22/nurse\\patient_22_0.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p24/nurse\\patient_24_234.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p26/nurse\\patient_26_100.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p27/nurse\\patient_27_10.png\n",
      "\n",
      "\n",
      "\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p28/nurse\\Pt_28_1_26.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p29/nurse\\Pt 29_1_173.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p30/nurse\\patient30_123.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p31/nurse\\Pt 31_109.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p32/nurse\\Patient_32_Video2_100.png\n",
      "\n",
      "\n",
      "\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p33/nurse\\patient33_119.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p35/nurse\\patient_35_1.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p36/nurse\\Patient_36_196.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p37/nurse\\Pt 37_1_223.png\n",
      "bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/p38/nurse\\patient_38_255.png\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_split_testing = kf.split(range(len(all_noone_train)))\n",
    "for train_index, val_index in cv_split_testing:\n",
    "    for i in val_index:\n",
    "        print(all_nurse_list[i][0])\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "noone_train: 11025, nurse_train: 1063\n",
      "noone_val: 3255, nurse_val: 297\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Zalamaan\\Documents\\Repos\\depthSliceTool\\InterventionDetection.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m model \u001b[39m=\u001b[39m create_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m model\u001b[39m.\u001b[39mload_weights(\u001b[39m\"\u001b[39m\u001b[39mSavedModels/cp_fold_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_gpu.ckpt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(foldNum_testing))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m evalModel(model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m foldNum_testing \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\Zalamaan\\Documents\\Repos\\depthSliceTool\\InterventionDetection.ipynb Cell 28\u001b[0m in \u001b[0;36mevalModel\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(noone_test_final)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     im \u001b[39m=\u001b[39m load_img(noone_test_final[i], color_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgrayscale\u001b[39m\u001b[39m'\u001b[39m, target_size\u001b[39m=\u001b[39m(IMAGE_HEIGHT, IMAGE_WIDTH))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     label \u001b[39m=\u001b[39m predictNurse(model, im)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     preds\u001b[39m.\u001b[39mappend(label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     truths\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mnoone\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Zalamaan\\Documents\\Repos\\depthSliceTool\\InterventionDetection.ipynb Cell 28\u001b[0m in \u001b[0;36mpredictNurse\u001b[1;34m(model, img)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x \u001b[39m=\u001b[39m img_to_array(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(x, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(x, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnurse\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m preds[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m>\u001b[39mpreds[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnoone\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m label\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv_split_testing = kf.split(range(len(all_noone_train)))\n",
    "foldNum_testing = 0\n",
    "for train_index, val_index in cv_split_testing:\n",
    "    print(\"Fold {}:\".format(foldNum_testing))\n",
    "\n",
    "    # Select the current folds for training and validation\n",
    "    noone_train = []\n",
    "    nurse_train = []\n",
    "    noone_val = []\n",
    "    nurse_val = []\n",
    "    for i in train_index:\n",
    "        noone_train += all_noone_train[i]\n",
    "        nurse_train += all_nurse_train[i]\n",
    "    for i in val_index:\n",
    "        noone_val += all_noone_train[i]\n",
    "        nurse_val += all_nurse_train[i]\n",
    "\n",
    "    # Move images to training and testing folders\n",
    "    noone_train, nurse_train, noone_val, nurse_val = prepFiles(noone_train, nurse_train, noone_val, nurse_val)\n",
    "    all_train = [noone_train, nurse_train]\n",
    "    all_val = [noone_val, nurse_val]\n",
    "\n",
    "    noone_test_final = glob(VAL_DIR + 'noone/*.png')\n",
    "    nurse_test_final = glob(VAL_DIR + 'nurse/*.png')\n",
    "\n",
    "\n",
    "    print(\"noone_train: {}, nurse_train: {}\".format(len(noone_train), len(nurse_train)))\n",
    "    print(\"noone_val: {}, nurse_val: {}\\n\".format(len(noone_val), len(nurse_val)))\n",
    "\n",
    "    model = create_model()\n",
    "    model.load_weights(\"SavedModels/cp_fold_{}_gpu.ckpt\".format(foldNum_testing))\n",
    "    \n",
    "    evalModel(model)\n",
    "    \n",
    "\n",
    "    foldNum_testing += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate no-weight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "evalModel(model)\n",
    "\n",
    "# model.load_weights(\"SavedModels/cp_fold_0.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate each of the 5-fold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    model.load_weights(\"SavedModels/cp_fold_{}.ckpt\".format(i))\n",
    "    evalModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain with all training data (no 5-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    \n",
    "    noone_train = []\n",
    "    for i in all_noone_train:\n",
    "        noone_train += i\n",
    "\n",
    "    nurse_train = []\n",
    "    for i in all_nurse_train:\n",
    "        nurse_train += i\n",
    "\n",
    "    # Move images to training and testing folders\n",
    "    noone_train, nurse_train, _, _ = prepFiles(noone_train, nurse_train, [], [])\n",
    "    all_train = [noone_train, nurse_train]\n",
    "\n",
    "    print(\"noone_train: {}, nurse_train: {}\".format(len(noone_train), len(nurse_train)))\n",
    "\n",
    "    weight_for_0 = (1 / (len(noone_train))) * ((len(nurse_train) + len(noone_train)) / 2.0)\n",
    "    weight_for_1 = (1 / (len(nurse_train))) * ((len(nurse_train) + len(noone_train)) / 2.0)\n",
    "    CLASS_WEIGHT = {0: weight_for_0, 1: weight_for_1}\n",
    "    print(\"Class Weights: {}\\n\".format(CLASS_WEIGHT))\n",
    "    \n",
    "    INITIAL_BIAS = np.log([(len(nurse_train)) / (len(noone_train))])\n",
    "    print(\"Initial Bias: {}\\n\".format(INITIAL_BIAS))\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=123,\n",
    "    )\n",
    "    \n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    checkpoint_path = \"SavedModels/cp_noFold.ckpt\"\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='prc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "    earlyStopping = EarlyStopping(monitor='prc', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "\n",
    "    history = model.fit(\n",
    "        x=train_generator,\n",
    "        epochs=20,\n",
    "        callbacks=[checkpoint, earlyStopping],\n",
    "        # class_weight=CLASS_WEIGHT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate NoFold Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.load_weights(\"SavedModels/cp_noFold.ckpt\")\n",
    "evalModel(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf8b9ac8caee8cd6ae392661a519e2d4fbbc720f8ec71fc2b548fd3ad16b33c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
