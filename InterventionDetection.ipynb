{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA_DIR = 'bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB/'\n",
    "TRAIN_DIR = ROOT_DATA_DIR + 'train/'\n",
    "TEST_DIR = ROOT_DATA_DIR + 'test/'\n",
    "VAL_DIR = ROOT_DATA_DIR + 'val/'\n",
    "TEST_SIZE = 0\n",
    "K_FOLDS = 5\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "CLASS_WEIGHT = None\n",
    "INITIAL_BIAS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import libdst\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import shutil\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Rescaling, Concatenate, Conv2D, Softmax, ReLU, Input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if len(physical_devices) > 0:\n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#     print(\"GPU memory growth enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noone_p1 = glob(ROOT_DATA_DIR + 'p1/noone/*.png')\n",
    "nurse_p1 = glob(ROOT_DATA_DIR + 'p1/nurse/*.png')\n",
    "noone_p2 = glob(ROOT_DATA_DIR + 'p2/noone/*.png')\n",
    "nurse_p2 = glob(ROOT_DATA_DIR + 'p2/nurse/*.png')\n",
    "noone_p5 = glob(ROOT_DATA_DIR + 'p5/noone/*.png')\n",
    "nurse_p5 = glob(ROOT_DATA_DIR + 'p5/nurse/*.png')\n",
    "noone_p6 = glob(ROOT_DATA_DIR + 'p6/noone/*.png')\n",
    "nurse_p6 = glob(ROOT_DATA_DIR + 'p6/nurse/*.png')\n",
    "noone_p8 = glob(ROOT_DATA_DIR + 'p8/noone/*.png')\n",
    "nurse_p8 = glob(ROOT_DATA_DIR + 'p8/nurse/*.png')\n",
    "noone_p9 = glob(ROOT_DATA_DIR + 'p9/noone/*.png')\n",
    "nurse_p9 = glob(ROOT_DATA_DIR + 'p9/nurse/*.png')\n",
    "noone_p10 = glob(ROOT_DATA_DIR + 'p10/noone/*.png')\n",
    "nurse_p10 = glob(ROOT_DATA_DIR + 'p10/nurse/*.png')\n",
    "noone_p11 = glob(ROOT_DATA_DIR + 'p11/noone/*.png')\n",
    "nurse_p11 = glob(ROOT_DATA_DIR + 'p11/nurse/*.png')\n",
    "noone_p13 = glob(ROOT_DATA_DIR + 'p13/noone/*.png')\n",
    "nurse_p13 = glob(ROOT_DATA_DIR + 'p13/nurse/*.png')\n",
    "noone_p14 = glob(ROOT_DATA_DIR + 'p14/noone/*.png')\n",
    "nurse_p14 = glob(ROOT_DATA_DIR + 'p14/nurse/*.png')\n",
    "noone_p15 = glob(ROOT_DATA_DIR + 'p15/noone/*.png')\n",
    "nurse_p15 = glob(ROOT_DATA_DIR + 'p15/nurse/*.png')\n",
    "noone_p16 = glob(ROOT_DATA_DIR + 'p16/noone/*.png')\n",
    "nurse_p16 = glob(ROOT_DATA_DIR + 'p16/nurse/*.png')\n",
    "noone_p17 = glob(ROOT_DATA_DIR + 'p17/noone/*.png')\n",
    "nurse_p17 = glob(ROOT_DATA_DIR + 'p17/nurse/*.png')\n",
    "# noone_p18 = glob(ROOT_DATA_DIR + 'p18/noone/*.png')\n",
    "# nurse_p18 = glob(ROOT_DATA_DIR + 'p18/nurse/*.png')\n",
    "# noone_p19 = glob(ROOT_DATA_DIR + 'p19/noone/*.png')\n",
    "# nurse_p19 = glob(ROOT_DATA_DIR + 'p19/nurse/*.png')\n",
    "noone_p21 = glob(ROOT_DATA_DIR + 'p21/noone/*.png')\n",
    "nurse_p21 = glob(ROOT_DATA_DIR + 'p21/nurse/*.png')\n",
    "noone_p22 = glob(ROOT_DATA_DIR + 'p22/noone/*.png')\n",
    "nurse_p22 = glob(ROOT_DATA_DIR + 'p22/nurse/*.png')\n",
    "noone_p23 = glob(ROOT_DATA_DIR + 'p23/noone/*.png')\n",
    "nurse_p23 = glob(ROOT_DATA_DIR + 'p23/nurse/*.png')\n",
    "noone_p24 = glob(ROOT_DATA_DIR + 'p24/noone/*.png')\n",
    "nurse_p24 = glob(ROOT_DATA_DIR + 'p24/nurse/*.png')\n",
    "noone_p25 = glob(ROOT_DATA_DIR + 'p25/noone/*.png')\n",
    "nurse_p25 = glob(ROOT_DATA_DIR + 'p25/nurse/*.png')\n",
    "noone_p26 = glob(ROOT_DATA_DIR + 'p26/noone/*.png')\n",
    "nurse_p26 = glob(ROOT_DATA_DIR + 'p26/nurse/*.png')\n",
    "noone_p27 = glob(ROOT_DATA_DIR + 'p27/noone/*.png')\n",
    "nurse_p27 = glob(ROOT_DATA_DIR + 'p27/nurse/*.png')\n",
    "noone_p28 = glob(ROOT_DATA_DIR + 'p28/noone/*.png')\n",
    "nurse_p28 = glob(ROOT_DATA_DIR + 'p28/nurse/*.png')\n",
    "noone_p29 = glob(ROOT_DATA_DIR + 'p29/noone/*.png')\n",
    "nurse_p29 = glob(ROOT_DATA_DIR + 'p29/nurse/*.png')\n",
    "noone_p30 = glob(ROOT_DATA_DIR + 'p30/noone/*.png')\n",
    "nurse_p30 = glob(ROOT_DATA_DIR + 'p30/nurse/*.png')\n",
    "noone_p31 = glob(ROOT_DATA_DIR + 'p31/noone/*.png')\n",
    "nurse_p31 = glob(ROOT_DATA_DIR + 'p31/nurse/*.png')\n",
    "noone_p32 = glob(ROOT_DATA_DIR + 'p32/noone/*.png')\n",
    "nurse_p32 = glob(ROOT_DATA_DIR + 'p32/nurse/*.png')\n",
    "noone_p33 = glob(ROOT_DATA_DIR + 'p33/noone/*.png')\n",
    "nurse_p33 = glob(ROOT_DATA_DIR + 'p33/nurse/*.png')\n",
    "noone_p34 = glob(ROOT_DATA_DIR + 'p34/noone/*.png')\n",
    "nurse_p34 = glob(ROOT_DATA_DIR + 'p34/nurse/*.png')\n",
    "# noone_p35 = glob(ROOT_DATA_DIR + 'p35/noone/*.png')\n",
    "# nurse_p35 = glob(ROOT_DATA_DIR + 'p35/nurse/*.png')\n",
    "# noone_p36 = glob(ROOT_DATA_DIR + 'p36/noone/*.png')\n",
    "# nurse_p36 = glob(ROOT_DATA_DIR + 'p36/nurse/*.png')\n",
    "# noone_p37 = glob(ROOT_DATA_DIR + 'p37/noone/*.png')\n",
    "# nurse_p37 = glob(ROOT_DATA_DIR + 'p37/nurse/*.png')\n",
    "# noone_p38 = glob(ROOT_DATA_DIR + 'p38/noone/*.png')\n",
    "# nurse_p38 = glob(ROOT_DATA_DIR + 'p38/nurse/*.png')\n",
    "\n",
    "# all_noone = noone_p1 + noone_p2 + noone_p5 + noone_p6 + noone_p8 + noone_p9 + noone_p10 + noone_p11 + noone_p13 + noone_p14 + noone_p15 + noone_p16 + noone_p17 + noone_p18 + noone_p19 + noone_p21 + noone_p22 + noone_p23 + noone_p24 + noone_p25 + noone_p26 + noone_p27 + noone_p28 + noone_p29 + noone_p30 + noone_p31 + noone_p32 + noone_p33 + noone_p34 + noone_p35 + noone_p36 + noone_p37 + noone_p38\n",
    "# all_nurse = nurse_p1 + nurse_p2 + nurse_p5 + nurse_p6 + nurse_p8 + nurse_p9 + nurse_p10 + nurse_p11 + nurse_p13 + nurse_p14 + nurse_p15 + nurse_p16 + nurse_p17 + nurse_p18 + nurse_p19 + nurse_p21 + nurse_p22 + nurse_p23 + nurse_p24 + nurse_p25 + nurse_p26 + nurse_p27 + nurse_p28 + nurse_p29 + nurse_p30 + nurse_p31 + nurse_p32 + nurse_p33 + nurse_p34 + nurse_p35 + nurse_p36 + nurse_p37 + nurse_p38\n",
    "\n",
    "# all_noone_list = [noone_p1, noone_p2, noone_p6, noone_p8, noone_p9, noone_p10, noone_p11, noone_p13, noone_p14, noone_p15, noone_p16, noone_p18, noone_p19, noone_p21, noone_p22, noone_p24, noone_p26, noone_p27, noone_p28, noone_p29, noone_p30, noone_p31, noone_p32, noone_p33, noone_p34, noone_p35, noone_p36, noone_p37, noone_p38]\n",
    "# all_nurse_list = [nurse_p1, nurse_p2, nurse_p6, nurse_p8, nurse_p9, nurse_p10, nurse_p11, nurse_p13, nurse_p14, nurse_p15, nurse_p16, nurse_p18, nurse_p19, nurse_p21, nurse_p22, nurse_p24, nurse_p26, nurse_p27, nurse_p28, nurse_p29, nurse_p30, nurse_p31, nurse_p32, nurse_p33, nurse_p34, nurse_p35, nurse_p36, nurse_p37, nurse_p38]\n",
    "\n",
    "# all_noone_list = [noone_p1, noone_p2, noone_p5, noone_p6, noone_p8, noone_p9, noone_p10, noone_p11, noone_p13, noone_p14, noone_p15, noone_p16, noone_p17, noone_p18, noone_p19, noone_p21, noone_p22, noone_p23, noone_p24, noone_p25, noone_p26, noone_p27, noone_p28, noone_p29, noone_p30, noone_p31, noone_p32, noone_p33, noone_p34, noone_p35, noone_p36, noone_p37, noone_p38]\n",
    "# all_nurse_list = [nurse_p1, nurse_p2, nurse_p5, nurse_p6, nurse_p8, nurse_p9, nurse_p10, nurse_p11, nurse_p13, nurse_p14, nurse_p15, nurse_p16, nurse_p17, nurse_p18, nurse_p19, nurse_p21, nurse_p22, nurse_p23, nurse_p24, nurse_p25, nurse_p26, nurse_p27, nurse_p28, nurse_p29, nurse_p30, nurse_p31, nurse_p32, nurse_p33, nurse_p34, nurse_p35, nurse_p36, nurse_p37, nurse_p38]\n",
    "\n",
    "all_noone_list = [noone_p1, noone_p2, noone_p5, noone_p6, noone_p8, noone_p9, noone_p10, noone_p11, noone_p13, noone_p14, noone_p15, noone_p16, noone_p17, noone_p21, noone_p22, noone_p23, noone_p24, noone_p25, noone_p26, noone_p27, noone_p28, noone_p29, noone_p30, noone_p31, noone_p32, noone_p33, noone_p34]\n",
    "all_nurse_list = [nurse_p1, nurse_p2, nurse_p5, nurse_p6, nurse_p8, nurse_p9, nurse_p10, nurse_p11, nurse_p13, nurse_p14, nurse_p15, nurse_p16, nurse_p17, nurse_p21, nurse_p22, nurse_p23, nurse_p24, nurse_p25, nurse_p26, nurse_p27, nurse_p28, nurse_p29, nurse_p30, nurse_p31, nurse_p32, nurse_p33, nurse_p34]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_noone_train = []\n",
    "# all_noone_test = []\n",
    "# for p in all_noone_list:\n",
    "#     if len(p) > 0:\n",
    "#         train, test = train_test_split(p, test_size=TEST_SIZE)\n",
    "#         all_noone_train += [train]\n",
    "#         all_noone_test += test\n",
    "\n",
    "# all_nurse_train = []\n",
    "# all_nurse_test = []\n",
    "# for p in all_nurse_list:\n",
    "#     if len(p) > 0:\n",
    "#         train, test = train_test_split(p, test_size=TEST_SIZE)\n",
    "#         all_nurse_train += [train]\n",
    "#         all_nurse_test += test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_noone_train = []\n",
    "all_noone_test = []\n",
    "all_nurse_train = []\n",
    "all_nurse_test = []\n",
    "\n",
    "if TEST_SIZE > 0:\n",
    "    train, test = train_test_split(range(len(all_noone_list)), test_size=TEST_SIZE)\n",
    "    \n",
    "    for i in train:\n",
    "        all_noone_train += [all_noone_list[i]]\n",
    "        all_nurse_train += [all_nurse_list[i]]\n",
    "\n",
    "    for i in test:\n",
    "        all_noone_test += all_noone_list[i]\n",
    "        all_nurse_test += all_nurse_list[i]\n",
    "elif TEST_SIZE == 0:\n",
    "    all_noone_train = all_noone_list\n",
    "    all_nurse_train = all_nurse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remFiles = glob(TEST_DIR + 'noone/'+ '*.png')\n",
    "for f in remFiles:\n",
    "    os.remove(f)\n",
    "\n",
    "remFiles = glob(TEST_DIR + 'nurse/'+ '*.png')\n",
    "for f in remFiles:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in all_noone_test:\n",
    "    basename = os.path.basename(f)\n",
    "    dst_path = TEST_DIR + 'noone/' + basename\n",
    "    shutil.copy(f, dst_path)\n",
    "\n",
    "for f in all_nurse_test:\n",
    "    basename = os.path.basename(f)\n",
    "    dst_path = TEST_DIR + 'nurse/' + basename\n",
    "    shutil.copy(f, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noone_test = glob(TEST_DIR + 'noone/*.png')\n",
    "nurse_test = glob(TEST_DIR + 'nurse/*.png')\n",
    "all_test = (noone_test, nurse_test)\n",
    "labels = ['noone', 'nurse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/65535,\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1/65535,\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1/65535,\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_batch, y_batch = next(test_generator)\n",
    "\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# plt.style.use('dark_background')\n",
    "# for k, (img, lbl) in enumerate(zip(x_batch, y_batch)):\n",
    "#     plt.subplot(4, 8, k+1)\n",
    "#     plt.imshow((img + 1) / 2)\n",
    "#     plt.title(\"Class: {}\".format(labels[int(lbl)]))\n",
    "#     plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = VGG16(weights='imagenet', include_top=True)\n",
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.layers[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_layers = test_model.layers[2:-1]\n",
    "# trans_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.layers\n",
    "# w = test_model.layers[1].get_weights()[0][:, :, 2, :]\n",
    "# w = w.reshape(3, 3, 1, 64)\n",
    "# b = test_model.layers[1].get_weights()[1]\n",
    "# first_conv2d = Conv2D(64, kernel_size=3, padding='same', kernel_initializer=tf.keras.initializers.zeros(), use_bias=True, activation='relu')\n",
    "\n",
    "# model = tf.keras.Sequential(\n",
    "#     [\n",
    "#         tf.keras.Input(shape=(224, 224, 1),name='input'),\n",
    "#         first_conv2d,\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         tf.keras.layers.Dense(10, activation=\"relu\",use_bias=True,bias_initializer='zeros',name='dense1'),\n",
    "#         tf.keras.layers.Dense(1, activation=\"softmax\",name='output'),\n",
    "#     ]\n",
    "# )\n",
    "# model.layers[0].get_weights()[0].shape\n",
    "\n",
    "# model.layers[0].set_weights([w, b])\n",
    "# model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.layers[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "        # tf.keras.metrics.TruePositives(name='tp'),\n",
    "        # tf.keras.metrics.FalsePositives(name='fp'),\n",
    "        # tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "        # tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "        # tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='AUC'),\n",
    "        tf.keras.metrics.AUC(name='prc', curve='PR'),\n",
    "    ]\n",
    "\n",
    "def create_model(finetuning=True, metrics=METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    \n",
    "    base_model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "    first_conv2d_bias = base_model.layers[1].get_weights()[-1]\n",
    "    first_conv2d_weights = base_model.layers[1].get_weights()[0][:, :, 2, :].reshape(3, 3, 1, 64)\n",
    "    \n",
    "    custom_input = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1), name=\"1-Channel_input\")\n",
    "    first_conv2d = Conv2D(64, kernel_size=3, padding='same', name=\"First_Conv2D\")\n",
    "    transfer_layers = base_model.layers[2:-1]\n",
    "    predictions = Dense(2, activation=\"softmax\", name=\"prediction_layer\", bias_initializer=output_bias)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(custom_input)\n",
    "    model.add(first_conv2d)\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.layers[0].set_weights([first_conv2d_weights, first_conv2d_bias])\n",
    "\n",
    "    for layer in transfer_layers:\n",
    "        # print(layer)\n",
    "        model.add(layer)\n",
    "\n",
    "    model.add(predictions)\n",
    "\n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "        # Use this for \n",
    "        optimizer=tf.optimizers.SGD(0.00001, momentum=0.9),\n",
    "        # optimizer=tf.optimizers.SGD(),\n",
    "        metrics=[METRICS],\n",
    "        )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepFiles(noone_train, nurse_train, noone_val, nurse_val):\n",
    "    # Clean up train and val folders\n",
    "    remFiles = glob(TRAIN_DIR + 'noone/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(TRAIN_DIR + 'nurse/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(VAL_DIR + 'noone/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(VAL_DIR + 'nurse/'+ '*.png')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Move files of current fold into their folders\n",
    "    for f in noone_train:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = TRAIN_DIR + 'noone/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in nurse_train:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = TRAIN_DIR + 'nurse/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in noone_val:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = VAL_DIR + 'noone/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in nurse_val:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = VAL_DIR + 'nurse/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "\n",
    "    noone_train = glob(TRAIN_DIR + 'noone/*.png')\n",
    "    nurse_train = glob(TRAIN_DIR + 'nurse/*.png')\n",
    "\n",
    "    noone_val = glob(VAL_DIR + 'noone/*.png')\n",
    "    nurse_val = glob(VAL_DIR + 'nurse/*.png')\n",
    "\n",
    "    return noone_train, nurse_train, noone_val, nurse_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: \n",
      "\n",
      "noone_train: 10734, nurse_train: 1001\n",
      "noone_val: 2910, nurse_val: 259\n",
      "\n",
      "Class Weights: {0: 0.5461741424802111, 1: 5.914285714285715}\n",
      "\n",
      "Initial Bias: [-2.38218814]\n",
      "\n",
      "Found 11735 images belonging to 2 classes.\n",
      "Found 3169 images belonging to 2 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "367/367 [==============================] - 2642s 7s/step - loss: 0.3260 - accuracy: 0.9035 - precision: 0.9035 - recall: 0.9035 - AUC: 0.9134 - prc: 0.8903\n",
      "Epoch 2/20\n",
      "367/367 [==============================] - 2645s 7s/step - loss: 0.2866 - accuracy: 0.9147 - precision: 0.9147 - recall: 0.9147 - AUC: 0.9317 - prc: 0.9204\n",
      "Epoch 3/20\n",
      "367/367 [==============================] - 2636s 7s/step - loss: 0.2826 - accuracy: 0.9147 - precision: 0.9147 - recall: 0.9147 - AUC: 0.9360 - prc: 0.9274\n",
      "Epoch 4/20\n",
      "367/367 [==============================] - 2644s 7s/step - loss: 0.2763 - accuracy: 0.9147 - precision: 0.9147 - recall: 0.9147 - AUC: 0.9428 - prc: 0.9352\n",
      "Epoch 5/20\n",
      "367/367 [==============================] - 2638s 7s/step - loss: 0.2698 - accuracy: 0.9147 - precision: 0.9147 - recall: 0.9147 - AUC: 0.9479 - prc: 0.9426\n",
      "Epoch 6/20\n",
      "367/367 [==============================] - 2637s 7s/step - loss: 0.2643 - accuracy: 0.9147 - precision: 0.9147 - recall: 0.9147 - AUC: 0.9513 - prc: 0.9457\n",
      "Epoch 7/20\n",
      "367/367 [==============================] - 2634s 7s/step - loss: 0.2610 - accuracy: 0.9147 - precision: 0.9147 - recall: 0.9147 - AUC: 0.9521 - prc: 0.9468\n",
      "Epoch 8/20\n",
      "367/367 [==============================] - 2638s 7s/step - loss: 0.2556 - accuracy: 0.9147 - precision: 0.9147 - recall: 0.9147 - AUC: 0.9551 - prc: 0.9505\n",
      "Epoch 9/20\n",
      "367/367 [==============================] - 2638s 7s/step - loss: 0.2514 - accuracy: 0.9150 - precision: 0.9150 - recall: 0.9150 - AUC: 0.9560 - prc: 0.9506\n",
      "Epoch 10/20\n",
      "367/367 [==============================] - 2641s 7s/step - loss: 0.2487 - accuracy: 0.9163 - precision: 0.9163 - recall: 0.9163 - AUC: 0.9569 - prc: 0.9518\n",
      "Epoch 11/20\n",
      "367/367 [==============================] - 2639s 7s/step - loss: 0.2476 - accuracy: 0.9167 - precision: 0.9167 - recall: 0.9167 - AUC: 0.9578 - prc: 0.9527\n",
      "Epoch 12/20\n",
      "367/367 [==============================] - 2640s 7s/step - loss: 0.2404 - accuracy: 0.9165 - precision: 0.9165 - recall: 0.9165 - AUC: 0.9605 - prc: 0.9561\n",
      "Epoch 13/20\n",
      "367/367 [==============================] - 2636s 7s/step - loss: 0.2349 - accuracy: 0.9170 - precision: 0.9170 - recall: 0.9170 - AUC: 0.9630 - prc: 0.9600\n",
      "Epoch 14/20\n",
      "367/367 [==============================] - 2640s 7s/step - loss: 0.2320 - accuracy: 0.9197 - precision: 0.9197 - recall: 0.9197 - AUC: 0.9636 - prc: 0.9599\n",
      "Epoch 15/20\n",
      "367/367 [==============================] - 2643s 7s/step - loss: 0.2213 - accuracy: 0.9227 - precision: 0.9227 - recall: 0.9227 - AUC: 0.9681 - prc: 0.9658\n",
      "Epoch 16/20\n",
      "367/367 [==============================] - 2643s 7s/step - loss: 0.2203 - accuracy: 0.9225 - precision: 0.9225 - recall: 0.9225 - AUC: 0.9681 - prc: 0.9661\n",
      "Epoch 17/20\n",
      "367/367 [==============================] - 2640s 7s/step - loss: 0.2169 - accuracy: 0.9225 - precision: 0.9225 - recall: 0.9225 - AUC: 0.9687 - prc: 0.9663\n",
      "Epoch 18/20\n",
      "367/367 [==============================] - 2645s 7s/step - loss: 0.2147 - accuracy: 0.9238 - precision: 0.9238 - recall: 0.9238 - AUC: 0.9698 - prc: 0.9675\n",
      "Epoch 19/20\n",
      "367/367 [==============================] - 2641s 7s/step - loss: 0.2111 - accuracy: 0.9254 - precision: 0.9254 - recall: 0.9254 - AUC: 0.9709 - prc: 0.9690\n",
      "Epoch 20/20\n",
      "367/367 [==============================] - 2652s 7s/step - loss: 0.2027 - accuracy: 0.9277 - precision: 0.9277 - recall: 0.9277 - AUC: 0.9735 - prc: 0.9721\n",
      "Fold 1: \n",
      "\n",
      "noone_train: 10428, nurse_train: 878\n",
      "noone_val: 3216, nurse_val: 382\n",
      "\n",
      "Class Weights: {0: 0.5461741424802111, 1: 5.914285714285715}\n",
      "\n",
      "Initial Bias: [-2.38218814]\n",
      "\n",
      "Found 11306 images belonging to 2 classes.\n",
      "Found 3598 images belonging to 2 classes.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "354/354 [==============================] - 2543s 7s/step - loss: 0.2773 - accuracy: 0.9250 - precision: 0.9250 - recall: 0.9250 - AUC: 0.9554 - prc: 0.9525\n",
      "Epoch 2/20\n",
      "354/354 [==============================] - 2548s 7s/step - loss: 0.2689 - accuracy: 0.9223 - precision: 0.9223 - recall: 0.9223 - AUC: 0.9371 - prc: 0.9237\n",
      "Epoch 3/20\n",
      "354/354 [==============================] - 2542s 7s/step - loss: 0.2657 - accuracy: 0.9223 - precision: 0.9223 - recall: 0.9223 - AUC: 0.9430 - prc: 0.9333\n",
      "Epoch 4/20\n",
      "354/354 [==============================] - 2544s 7s/step - loss: 0.2610 - accuracy: 0.9223 - precision: 0.9223 - recall: 0.9223 - AUC: 0.9473 - prc: 0.9396\n",
      "Epoch 5/20\n",
      "354/354 [==============================] - 2547s 7s/step - loss: 0.2563 - accuracy: 0.9223 - precision: 0.9223 - recall: 0.9223 - AUC: 0.9502 - prc: 0.9417\n",
      "Epoch 6/20\n",
      "354/354 [==============================] - 2545s 7s/step - loss: 0.2519 - accuracy: 0.9223 - precision: 0.9223 - recall: 0.9223 - AUC: 0.9528 - prc: 0.9446\n",
      "Epoch 7/20\n",
      "354/354 [==============================] - 2556s 7s/step - loss: 0.2462 - accuracy: 0.9223 - precision: 0.9223 - recall: 0.9223 - AUC: 0.9561 - prc: 0.9495\n",
      "Epoch 8/20\n",
      "354/354 [==============================] - 2546s 7s/step - loss: 0.2434 - accuracy: 0.9223 - precision: 0.9223 - recall: 0.9223 - AUC: 0.9567 - prc: 0.9498\n",
      "Epoch 9/20\n",
      "354/354 [==============================] - 2548s 7s/step - loss: 0.2364 - accuracy: 0.9223 - precision: 0.9223 - recall: 0.9223 - AUC: 0.9599 - prc: 0.9553\n",
      "Epoch 10/20\n",
      "354/354 [==============================] - 2546s 7s/step - loss: 0.2289 - accuracy: 0.9224 - precision: 0.9224 - recall: 0.9224 - AUC: 0.9631 - prc: 0.9584\n",
      "Epoch 11/20\n",
      "354/354 [==============================] - 2551s 7s/step - loss: 0.2229 - accuracy: 0.9232 - precision: 0.9232 - recall: 0.9232 - AUC: 0.9653 - prc: 0.9611\n",
      "Epoch 12/20\n",
      "354/354 [==============================] - 2548s 7s/step - loss: 0.2154 - accuracy: 0.9247 - precision: 0.9247 - recall: 0.9247 - AUC: 0.9681 - prc: 0.9643\n",
      "Epoch 13/20\n",
      "354/354 [==============================] - 2548s 7s/step - loss: 0.2134 - accuracy: 0.9268 - precision: 0.9268 - recall: 0.9268 - AUC: 0.9685 - prc: 0.9664\n",
      "Epoch 14/20\n",
      "354/354 [==============================] - 2555s 7s/step - loss: 0.2070 - accuracy: 0.9292 - precision: 0.9292 - recall: 0.9292 - AUC: 0.9699 - prc: 0.9670\n",
      "Epoch 15/20\n",
      "354/354 [==============================] - 2547s 7s/step - loss: 0.2013 - accuracy: 0.9299 - precision: 0.9299 - recall: 0.9299 - AUC: 0.9722 - prc: 0.9700\n",
      "Epoch 16/20\n",
      "354/354 [==============================] - 2551s 7s/step - loss: 0.1895 - accuracy: 0.9331 - precision: 0.9331 - recall: 0.9331 - AUC: 0.9751 - prc: 0.9720\n",
      "Epoch 17/20\n",
      "354/354 [==============================] - 2551s 7s/step - loss: 0.1861 - accuracy: 0.9349 - precision: 0.9349 - recall: 0.9349 - AUC: 0.9762 - prc: 0.9747\n",
      "Epoch 18/20\n",
      "354/354 [==============================] - 2558s 7s/step - loss: 0.1883 - accuracy: 0.9384 - precision: 0.9384 - recall: 0.9384 - AUC: 0.9746 - prc: 0.9714\n",
      "Epoch 19/20\n",
      "354/354 [==============================] - 2549s 7s/step - loss: 0.1800 - accuracy: 0.9368 - precision: 0.9368 - recall: 0.9368 - AUC: 0.9775 - prc: 0.9761\n",
      "Epoch 20/20\n",
      "354/354 [==============================] - 2552s 7s/step - loss: 0.1748 - accuracy: 0.9378 - precision: 0.9378 - recall: 0.9378 - AUC: 0.9790 - prc: 0.9775\n",
      "Fold 2: \n",
      "\n",
      "noone_train: 11126, nurse_train: 1042\n",
      "noone_val: 2518, nurse_val: 218\n",
      "\n",
      "Class Weights: {0: 0.5461741424802111, 1: 5.914285714285715}\n",
      "\n",
      "Initial Bias: [-2.38218814]\n",
      "\n",
      "Found 12168 images belonging to 2 classes.\n",
      "Found 2736 images belonging to 2 classes.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "381/381 [==============================] - 2744s 7s/step - loss: 0.3094 - accuracy: 0.9212 - precision: 0.9212 - recall: 0.9212 - AUC: 0.9533 - prc: 0.9495\n",
      "Epoch 2/20\n",
      "381/381 [==============================] - 2747s 7s/step - loss: 0.2916 - accuracy: 0.9144 - precision: 0.9144 - recall: 0.9144 - AUC: 0.9211 - prc: 0.9036\n",
      "Epoch 3/20\n",
      "381/381 [==============================] - 2748s 7s/step - loss: 0.2899 - accuracy: 0.9144 - precision: 0.9144 - recall: 0.9144 - AUC: 0.9257 - prc: 0.9106\n",
      "Epoch 4/20\n",
      "381/381 [==============================] - 2753s 7s/step - loss: 0.2871 - accuracy: 0.9144 - precision: 0.9144 - recall: 0.9144 - AUC: 0.9308 - prc: 0.9203\n",
      "Epoch 5/20\n",
      "381/381 [==============================] - 2751s 7s/step - loss: 0.2829 - accuracy: 0.9144 - precision: 0.9144 - recall: 0.9144 - AUC: 0.9392 - prc: 0.9320\n",
      "Epoch 6/20\n",
      "381/381 [==============================] - 2756s 7s/step - loss: 0.2781 - accuracy: 0.9144 - precision: 0.9144 - recall: 0.9144 - AUC: 0.9433 - prc: 0.9350\n",
      "Epoch 7/20\n",
      "381/381 [==============================] - 2750s 7s/step - loss: 0.2695 - accuracy: 0.9144 - precision: 0.9144 - recall: 0.9144 - AUC: 0.9506 - prc: 0.9454\n",
      "Epoch 8/20\n",
      "381/381 [==============================] - 2752s 7s/step - loss: 0.2669 - accuracy: 0.9144 - precision: 0.9144 - recall: 0.9144 - AUC: 0.9491 - prc: 0.9432\n",
      "Epoch 9/20\n",
      "381/381 [==============================] - 2746s 7s/step - loss: 0.2592 - accuracy: 0.9144 - precision: 0.9144 - recall: 0.9144 - AUC: 0.9547 - prc: 0.9492\n",
      "Epoch 10/20\n",
      "381/381 [==============================] - 2756s 7s/step - loss: 0.2527 - accuracy: 0.9144 - precision: 0.9144 - recall: 0.9144 - AUC: 0.9573 - prc: 0.9532\n",
      "Epoch 11/20\n",
      "381/381 [==============================] - 2746s 7s/step - loss: 0.2461 - accuracy: 0.9144 - precision: 0.9144 - recall: 0.9144 - AUC: 0.9602 - prc: 0.9571\n",
      "Epoch 12/20\n",
      "381/381 [==============================] - 2753s 7s/step - loss: 0.2413 - accuracy: 0.9154 - precision: 0.9154 - recall: 0.9154 - AUC: 0.9613 - prc: 0.9581\n",
      "Epoch 13/20\n",
      "381/381 [==============================] - 2754s 7s/step - loss: 0.2370 - accuracy: 0.9153 - precision: 0.9153 - recall: 0.9153 - AUC: 0.9633 - prc: 0.9604\n",
      "Epoch 14/20\n",
      "381/381 [==============================] - 2759s 7s/step - loss: 0.2296 - accuracy: 0.9188 - precision: 0.9188 - recall: 0.9188 - AUC: 0.9656 - prc: 0.9636\n",
      "Epoch 15/20\n",
      "381/381 [==============================] - 2765s 7s/step - loss: 0.2277 - accuracy: 0.9174 - precision: 0.9174 - recall: 0.9174 - AUC: 0.9661 - prc: 0.9635\n",
      "Epoch 16/20\n",
      "381/381 [==============================] - 2753s 7s/step - loss: 0.2237 - accuracy: 0.9190 - precision: 0.9190 - recall: 0.9190 - AUC: 0.9670 - prc: 0.9644\n",
      "Epoch 17/20\n",
      "381/381 [==============================] - 2747s 7s/step - loss: 0.2153 - accuracy: 0.9224 - precision: 0.9224 - recall: 0.9224 - AUC: 0.9701 - prc: 0.9687\n",
      "Epoch 18/20\n",
      "381/381 [==============================] - 2758s 7s/step - loss: 0.2145 - accuracy: 0.9222 - precision: 0.9222 - recall: 0.9222 - AUC: 0.9697 - prc: 0.9679\n",
      "Epoch 19/20\n",
      "381/381 [==============================] - 2749s 7s/step - loss: 0.2097 - accuracy: 0.9252 - precision: 0.9252 - recall: 0.9252 - AUC: 0.9713 - prc: 0.9697\n",
      "Epoch 20/20\n",
      "381/381 [==============================] - 2753s 7s/step - loss: 0.2029 - accuracy: 0.9265 - precision: 0.9265 - recall: 0.9265 - AUC: 0.9735 - prc: 0.9726\n",
      "Fold 3: \n",
      "\n",
      "noone_train: 11026, nurse_train: 1027\n",
      "noone_val: 2618, nurse_val: 233\n",
      "\n",
      "Class Weights: {0: 0.5461741424802111, 1: 5.914285714285715}\n",
      "\n",
      "Initial Bias: [-2.38218814]\n",
      "\n",
      "Found 12053 images belonging to 2 classes.\n",
      "Found 2851 images belonging to 2 classes.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "377/377 [==============================] - 2708s 7s/step - loss: 0.2894 - accuracy: 0.9207 - precision: 0.9207 - recall: 0.9207 - AUC: 0.9555 - prc: 0.9530\n",
      "Epoch 2/20\n",
      "377/377 [==============================] - 2721s 7s/step - loss: 0.2825 - accuracy: 0.9148 - precision: 0.9148 - recall: 0.9148 - AUC: 0.9380 - prc: 0.9283\n",
      "Epoch 3/20\n",
      "377/377 [==============================] - 2715s 7s/step - loss: 0.2745 - accuracy: 0.9148 - precision: 0.9148 - recall: 0.9148 - AUC: 0.9453 - prc: 0.9380\n",
      "Epoch 4/20\n",
      "377/377 [==============================] - 2717s 7s/step - loss: 0.2671 - accuracy: 0.9148 - precision: 0.9148 - recall: 0.9148 - AUC: 0.9497 - prc: 0.9439\n",
      "Epoch 5/20\n",
      "377/377 [==============================] - 2717s 7s/step - loss: 0.2617 - accuracy: 0.9148 - precision: 0.9148 - recall: 0.9148 - AUC: 0.9517 - prc: 0.9447\n",
      "Epoch 6/20\n",
      "377/377 [==============================] - 2729s 7s/step - loss: 0.2543 - accuracy: 0.9148 - precision: 0.9148 - recall: 0.9148 - AUC: 0.9555 - prc: 0.9498\n",
      "Epoch 7/20\n",
      "377/377 [==============================] - 2712s 7s/step - loss: 0.2434 - accuracy: 0.9150 - precision: 0.9150 - recall: 0.9150 - AUC: 0.9604 - prc: 0.9560\n",
      "Epoch 8/20\n",
      "377/377 [==============================] - 2724s 7s/step - loss: 0.2386 - accuracy: 0.9160 - precision: 0.9160 - recall: 0.9160 - AUC: 0.9612 - prc: 0.9572\n",
      "Epoch 9/20\n",
      "377/377 [==============================] - 2714s 7s/step - loss: 0.2407 - accuracy: 0.9188 - precision: 0.9188 - recall: 0.9188 - AUC: 0.9595 - prc: 0.9538\n",
      "Epoch 10/20\n",
      "377/377 [==============================] - 2720s 7s/step - loss: 0.2250 - accuracy: 0.9214 - precision: 0.9214 - recall: 0.9214 - AUC: 0.9663 - prc: 0.9632\n",
      "Epoch 11/20\n",
      "377/377 [==============================] - 2716s 7s/step - loss: 0.2278 - accuracy: 0.9223 - precision: 0.9223 - recall: 0.9223 - AUC: 0.9641 - prc: 0.9600\n",
      "Epoch 12/20\n",
      "377/377 [==============================] - 2725s 7s/step - loss: 0.2214 - accuracy: 0.9233 - precision: 0.9233 - recall: 0.9233 - AUC: 0.9671 - prc: 0.9651\n",
      "Epoch 13/20\n",
      "377/377 [==============================] - 2718s 7s/step - loss: 0.2149 - accuracy: 0.9252 - precision: 0.9252 - recall: 0.9252 - AUC: 0.9694 - prc: 0.9671\n",
      "Epoch 14/20\n",
      "377/377 [==============================] - 2714s 7s/step - loss: 0.2094 - accuracy: 0.9276 - precision: 0.9276 - recall: 0.9276 - AUC: 0.9707 - prc: 0.9688\n",
      "Epoch 15/20\n",
      "377/377 [==============================] - 2716s 7s/step - loss: 0.2043 - accuracy: 0.9286 - precision: 0.9286 - recall: 0.9286 - AUC: 0.9728 - prc: 0.9717\n",
      "Epoch 16/20\n",
      "377/377 [==============================] - 2715s 7s/step - loss: 0.1962 - accuracy: 0.9298 - precision: 0.9298 - recall: 0.9298 - AUC: 0.9752 - prc: 0.9742\n",
      "Epoch 17/20\n",
      "377/377 [==============================] - 2743s 7s/step - loss: 0.1950 - accuracy: 0.9311 - precision: 0.9311 - recall: 0.9311 - AUC: 0.9752 - prc: 0.9741\n",
      "Epoch 18/20\n",
      "377/377 [==============================] - 2714s 7s/step - loss: 0.1897 - accuracy: 0.9326 - precision: 0.9326 - recall: 0.9326 - AUC: 0.9765 - prc: 0.9755\n",
      "Epoch 19/20\n",
      "377/377 [==============================] - 2712s 7s/step - loss: 0.1867 - accuracy: 0.9324 - precision: 0.9324 - recall: 0.9324 - AUC: 0.9779 - prc: 0.9776\n",
      "Epoch 20/20\n",
      "377/377 [==============================] - 2714s 7s/step - loss: 0.1895 - accuracy: 0.9316 - precision: 0.9316 - recall: 0.9316 - AUC: 0.9765 - prc: 0.9750\n",
      "Fold 4: \n",
      "\n",
      "noone_train: 11262, nurse_train: 1092\n",
      "noone_val: 2382, nurse_val: 168\n",
      "\n",
      "Class Weights: {0: 0.5461741424802111, 1: 5.914285714285715}\n",
      "\n",
      "Initial Bias: [-2.38218814]\n",
      "\n",
      "Found 12354 images belonging to 2 classes.\n",
      "Found 2550 images belonging to 2 classes.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "387/387 [==============================] - 2798s 7s/step - loss: 0.3029 - accuracy: 0.9210 - precision: 0.9210 - recall: 0.9210 - AUC: 0.9543 - prc: 0.9517\n",
      "Epoch 2/20\n",
      "387/387 [==============================] - 2782s 7s/step - loss: 0.2945 - accuracy: 0.9116 - precision: 0.9116 - recall: 0.9116 - AUC: 0.9278 - prc: 0.9145\n",
      "Epoch 3/20\n",
      "387/387 [==============================] - 2790s 7s/step - loss: 0.2864 - accuracy: 0.9116 - precision: 0.9116 - recall: 0.9116 - AUC: 0.9407 - prc: 0.9329\n",
      "Epoch 4/20\n",
      "387/387 [==============================] - 2782s 7s/step - loss: 0.2797 - accuracy: 0.9116 - precision: 0.9116 - recall: 0.9116 - AUC: 0.9443 - prc: 0.9387\n",
      "Epoch 5/20\n",
      "387/387 [==============================] - 2792s 7s/step - loss: 0.2675 - accuracy: 0.9116 - precision: 0.9116 - recall: 0.9116 - AUC: 0.9526 - prc: 0.9491\n",
      "Epoch 6/20\n",
      "387/387 [==============================] - 2800s 7s/step - loss: 0.2594 - accuracy: 0.9116 - precision: 0.9116 - recall: 0.9116 - AUC: 0.9556 - prc: 0.9525\n",
      "Epoch 7/20\n",
      "387/387 [==============================] - 2795s 7s/step - loss: 0.2514 - accuracy: 0.9117 - precision: 0.9117 - recall: 0.9117 - AUC: 0.9586 - prc: 0.9561\n",
      "Epoch 8/20\n",
      "387/387 [==============================] - 2784s 7s/step - loss: 0.2413 - accuracy: 0.9132 - precision: 0.9132 - recall: 0.9132 - AUC: 0.9616 - prc: 0.9591\n",
      "Epoch 9/20\n",
      "387/387 [==============================] - 2790s 7s/step - loss: 0.2312 - accuracy: 0.9176 - precision: 0.9176 - recall: 0.9176 - AUC: 0.9647 - prc: 0.9619\n",
      "Epoch 10/20\n",
      "387/387 [==============================] - 2785s 7s/step - loss: 0.2202 - accuracy: 0.9211 - precision: 0.9211 - recall: 0.9211 - AUC: 0.9684 - prc: 0.9668\n",
      "Epoch 11/20\n",
      "387/387 [==============================] - 2797s 7s/step - loss: 0.2101 - accuracy: 0.9243 - precision: 0.9243 - recall: 0.9243 - AUC: 0.9716 - prc: 0.9696\n",
      "Epoch 12/20\n",
      "387/387 [==============================] - 2781s 7s/step - loss: 0.2072 - accuracy: 0.9250 - precision: 0.9250 - recall: 0.9250 - AUC: 0.9724 - prc: 0.9711\n",
      "Epoch 13/20\n",
      "387/387 [==============================] - 2785s 7s/step - loss: 0.2013 - accuracy: 0.9264 - precision: 0.9264 - recall: 0.9264 - AUC: 0.9738 - prc: 0.9725\n",
      "Epoch 14/20\n",
      "387/387 [==============================] - 2787s 7s/step - loss: 0.1982 - accuracy: 0.9272 - precision: 0.9272 - recall: 0.9272 - AUC: 0.9748 - prc: 0.9737\n",
      "Epoch 15/20\n",
      "387/387 [==============================] - 2795s 7s/step - loss: 0.1956 - accuracy: 0.9298 - precision: 0.9298 - recall: 0.9298 - AUC: 0.9753 - prc: 0.9743\n",
      "Epoch 16/20\n",
      "387/387 [==============================] - 2793s 7s/step - loss: 0.1858 - accuracy: 0.9337 - precision: 0.9337 - recall: 0.9337 - AUC: 0.9775 - prc: 0.9766\n",
      "Epoch 17/20\n",
      "387/387 [==============================] - 2796s 7s/step - loss: 0.1850 - accuracy: 0.9327 - precision: 0.9327 - recall: 0.9327 - AUC: 0.9776 - prc: 0.9768\n",
      "Epoch 18/20\n",
      "387/387 [==============================] - 2789s 7s/step - loss: 0.1722 - accuracy: 0.9378 - precision: 0.9378 - recall: 0.9378 - AUC: 0.9809 - prc: 0.9799\n",
      "Epoch 19/20\n",
      "387/387 [==============================] - 2791s 7s/step - loss: 0.1767 - accuracy: 0.9369 - precision: 0.9369 - recall: 0.9369 - AUC: 0.9795 - prc: 0.9787\n",
      "Epoch 20/20\n",
      "387/387 [==============================] - 2788s 7s/step - loss: 0.1680 - accuracy: 0.9385 - precision: 0.9385 - recall: 0.9385 - AUC: 0.9815 - prc: 0.9805\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=K_FOLDS)\n",
    "cv_split = kf.split(range(len(all_noone_train)))\n",
    "foldNum = 0\n",
    "\n",
    "foldHistories = []\n",
    "# foldMetrics = []\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    for train_index, val_index in cv_split:\n",
    "        print(\"Fold {}: \\n\".format(foldNum))\n",
    "\n",
    "        # Select the current folds for training and validation\n",
    "        noone_train = []\n",
    "        nurse_train = []\n",
    "        noone_val = []\n",
    "        nurse_val = []\n",
    "        for i in train_index:\n",
    "            noone_train += all_noone_train[i]\n",
    "            nurse_train += all_nurse_train[i]\n",
    "        for i in val_index:\n",
    "            noone_val += all_noone_train[i]\n",
    "            nurse_val += all_nurse_train[i]\n",
    "\n",
    "        # Move images to training and testing folders\n",
    "        noone_train, nurse_train, noone_val, nurse_val = prepFiles(noone_train, nurse_train, noone_val, nurse_val)\n",
    "        all_train = [noone_train, nurse_train]\n",
    "        all_val = [noone_val, nurse_val]\n",
    "\n",
    "        print(\"noone_train: {}, nurse_train: {}\".format(len(noone_train), len(nurse_train)))\n",
    "        print(\"noone_val: {}, nurse_val: {}\\n\".format(len(noone_val), len(nurse_val)))\n",
    "\n",
    "        weight_for_0 = (1 / (len(noone_train) + len(noone_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "        weight_for_1 = (1 / (len(nurse_train) + len(nurse_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "        CLASS_WEIGHT = {0: weight_for_0, 1: weight_for_1}\n",
    "        print(\"Class Weights: {}\\n\".format(CLASS_WEIGHT))\n",
    "        \n",
    "        INITIAL_BIAS = np.log([(len(nurse_train) + len(nurse_val)) / (len(noone_train) + len(noone_val))])\n",
    "        print(\"Initial Bias: {}\\n\".format(INITIAL_BIAS))\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            TRAIN_DIR,\n",
    "            target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "            color_mode='grayscale',\n",
    "            class_mode='categorical',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            seed=123,\n",
    "        )\n",
    "        val_generator = val_datagen.flow_from_directory(\n",
    "            VAL_DIR,\n",
    "            target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "            color_mode='grayscale',\n",
    "            class_mode='categorical',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            seed=123,\n",
    "        )\n",
    "\n",
    "        # model = create_model(output_bias=INITIAL_BIAS)\n",
    "        model = create_model()\n",
    "        model.summary()\n",
    "        checkpoint_path = \"SavedModels/cp_fold_{}_lab.ckpt\".format(foldNum)\n",
    "        checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_prc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "        earlyStopping = EarlyStopping(monitor='prc', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "\n",
    "        history = model.fit(\n",
    "            x=train_generator,\n",
    "            epochs=20,\n",
    "            # validation_data=val_generator,\n",
    "            # validation_steps=1,\n",
    "            # callbacks=[checkpoint],\n",
    "            # callbacks=[checkpoint, earlyStopping],\n",
    "            # class_weight=CLASS_WEIGHT,\n",
    "        )\n",
    "\n",
    "        model.save_weights(checkpoint_path)\n",
    "\n",
    "        foldHistories.append(history)\n",
    "\n",
    "        # metrics = model.evaluate(test_generator, return_dict=True)\n",
    "        # for name, value in metrics.items():\n",
    "        #     print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "        # foldMetrics.append(metrics)\n",
    "\n",
    "        foldNum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set includes 0 negatives and 0 positives\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noone_test_final = glob(TEST_DIR + 'noone/*.png')\n",
    "nurse_test_final = glob(TEST_DIR + 'nurse/*.png')\n",
    "print(\"Test set includes {} negatives and {} positives\".format(len(noone_test_final), len(nurse_test_final)))\n",
    "\n",
    "def printCM(cm, labels):\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax=ax)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticklabels(labels)\n",
    "    return\n",
    "\n",
    "def predictNurse(model, img):\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    preds = model.predict(x, verbose=0)\n",
    "    label = 'nurse' if preds[0][1]>preds[0][0] else 'noone'\n",
    "    return label\n",
    "\n",
    "def evalModel(model):\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for i in range(len(noone_test_final)):\n",
    "        im = load_img(noone_test_final[i], color_mode='grayscale', target_size=(IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        label = predictNurse(model, im)\n",
    "        preds.append(label)\n",
    "        truths.append('noone')\n",
    "\n",
    "    for i in range(len(nurse_test_final)):\n",
    "        im = load_img(nurse_test_final[i], color_mode='grayscale', target_size=(IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        label = predictNurse(model, im)\n",
    "        preds.append(label)\n",
    "        truths.append('nurse')\n",
    "\n",
    "    labels = ['noone', 'nurse']\n",
    "    cm = confusion_matrix(truths, preds)\n",
    "    printCM(cm, labels)\n",
    "\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TP = cm[1][1]\n",
    "    prec = TP / (TP + FP)\n",
    "    spec = TN / (TN + FP)\n",
    "    sens = TP / (TP + FN)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    f1 = (2 * TP) / ((2 * TP) + FP + FN)\n",
    "    print(\"Sens: {}, Spec: {}, Prec: {}, Acc: {}, F1: {}\".format(sens, spec, prec, acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_split_testing = kf.split(range(len(all_noone_train)))\n",
    "# for train_index, val_index in cv_split_testing:\n",
    "#     for i in val_index:\n",
    "#         print(all_nurse_list[i][0])\n",
    "#     print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "noone_train: 10734, nurse_train: 1001\n",
      "noone_val: 2910, nurse_val: 259\n",
      "\n",
      "Sens: 0.22007722007722008, Spec: 0.8824742268041237, Prec: 0.14285714285714285, Acc: 0.828337014831177, F1: 0.17325227963525835\n",
      "Fold 1:\n",
      "noone_train: 10428, nurse_train: 878\n",
      "noone_val: 3216, nurse_val: 382\n",
      "\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n",
      "Sens: 0.7513089005235603, Spec: 0.2325870646766169, Prec: 0.10417422867513612, Acc: 0.2876598110061145, F1: 0.18297736691106153\n",
      "Fold 2:\n",
      "noone_train: 11126, nurse_train: 1042\n",
      "noone_val: 2518, nurse_val: 218\n",
      "\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n",
      "Sens: 0.5275229357798165, Spec: 0.34511517077045273, Prec: 0.06519274376417233, Acc: 0.35964912280701755, F1: 0.11604439959636731\n",
      "Fold 3:\n",
      "noone_train: 11026, nurse_train: 1027\n",
      "noone_val: 2618, nurse_val: 233\n",
      "\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n",
      "Sens: 0.2145922746781116, Spec: 0.5084033613445378, Prec: 0.037397157816005985, Acc: 0.4843914415994388, F1: 0.06369426751592357\n",
      "Fold 4:\n",
      "noone_train: 11262, nurse_train: 1092\n",
      "noone_val: 2382, nurse_val: 168\n",
      "\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n",
      "Sens: 0.5595238095238095, Spec: 0.5356842989084802, Prec: 0.07833333333333334, Acc: 0.5372549019607843, F1: 0.13742690058479531\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_split_testing = kf.split(range(len(all_noone_train)))\n",
    "foldNum_testing = 0\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    for train_index, val_index in cv_split_testing:\n",
    "        print(\"Fold {}:\".format(foldNum_testing))\n",
    "\n",
    "        # Select the current folds for training and validation\n",
    "        noone_train = []\n",
    "        nurse_train = []\n",
    "        noone_val = []\n",
    "        nurse_val = []\n",
    "        for i in train_index:\n",
    "            noone_train += all_noone_train[i]\n",
    "            nurse_train += all_nurse_train[i]\n",
    "        for i in val_index:\n",
    "            noone_val += all_noone_train[i]\n",
    "            nurse_val += all_nurse_train[i]\n",
    "\n",
    "        # Move images to training and testing folders\n",
    "        noone_train, nurse_train, noone_val, nurse_val = prepFiles(noone_train, nurse_train, noone_val, nurse_val)\n",
    "        all_train = [noone_train, nurse_train]\n",
    "        all_val = [noone_val, nurse_val]\n",
    "\n",
    "        noone_test_final = glob(VAL_DIR + 'noone/*.png')\n",
    "        nurse_test_final = glob(VAL_DIR + 'nurse/*.png')\n",
    "\n",
    "\n",
    "        print(\"noone_train: {}, nurse_train: {}\".format(len(noone_train), len(nurse_train)))\n",
    "        print(\"noone_val: {}, nurse_val: {}\\n\".format(len(noone_val), len(nurse_val)))\n",
    "\n",
    "        model = create_model()\n",
    "        model.load_weights(\"SavedModels/cp_fold_{}_lab.ckpt\".format(foldNum_testing))\n",
    "        \n",
    "        evalModel(model)\n",
    "        \n",
    "\n",
    "        foldNum_testing += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate no-weight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "evalModel(model)\n",
    "\n",
    "# model.load_weights(\"SavedModels/cp_fold_0.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate each of the 5-fold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    model.load_weights(\"SavedModels/cp_fold_{}.ckpt\".format(i))\n",
    "    evalModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain with all training data (no 5-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    \n",
    "    noone_train = []\n",
    "    for i in all_noone_train:\n",
    "        noone_train += i\n",
    "\n",
    "    nurse_train = []\n",
    "    for i in all_nurse_train:\n",
    "        nurse_train += i\n",
    "\n",
    "    # Move images to training and testing folders\n",
    "    noone_train, nurse_train, _, _ = prepFiles(noone_train, nurse_train, [], [])\n",
    "    all_train = [noone_train, nurse_train]\n",
    "\n",
    "    print(\"noone_train: {}, nurse_train: {}\".format(len(noone_train), len(nurse_train)))\n",
    "\n",
    "    weight_for_0 = (1 / (len(noone_train))) * ((len(nurse_train) + len(noone_train)) / 2.0)\n",
    "    weight_for_1 = (1 / (len(nurse_train))) * ((len(nurse_train) + len(noone_train)) / 2.0)\n",
    "    CLASS_WEIGHT = {0: weight_for_0, 1: weight_for_1}\n",
    "    print(\"Class Weights: {}\\n\".format(CLASS_WEIGHT))\n",
    "    \n",
    "    INITIAL_BIAS = np.log([(len(nurse_train)) / (len(noone_train))])\n",
    "    print(\"Initial Bias: {}\\n\".format(INITIAL_BIAS))\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=123,\n",
    "    )\n",
    "    \n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    checkpoint_path = \"SavedModels/cp_noFold.ckpt\"\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='prc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "    earlyStopping = EarlyStopping(monitor='prc', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "\n",
    "    history = model.fit(\n",
    "        x=train_generator,\n",
    "        epochs=20,\n",
    "        callbacks=[checkpoint, earlyStopping],\n",
    "        # class_weight=CLASS_WEIGHT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate NoFold Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.load_weights(\"SavedModels/cp_noFold.ckpt\")\n",
    "evalModel(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 | packaged by conda-forge | (default, Sep 13 2021, 21:12:34) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "feaffe1decdce6da6bf9436be4bd5d5b58284f2856fcc6b5f6f641c092534381"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
