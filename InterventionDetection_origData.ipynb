{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA_DIR = 'bagmerge/InterventionDetectionFiles/FirstFrameDepthRGB_origData/'\n",
    "TRAIN_DIR = ROOT_DATA_DIR + 'train/'\n",
    "TEST_DIR = ROOT_DATA_DIR + 'test/'\n",
    "VAL_DIR = ROOT_DATA_DIR + 'val/'\n",
    "TEST_SIZE = 0\n",
    "K_FOLDS = 5\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "CLASS_WEIGHT = None\n",
    "INITIAL_BIAS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import libdst\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import shutil\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Rescaling, Concatenate, Conv2D, Softmax, ReLU, Input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory growth enabled\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"GPU memory growth enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noone_p1 = glob(ROOT_DATA_DIR + 'p1/noone/*.jpg')\n",
    "nurse_p1 = glob(ROOT_DATA_DIR + 'p1/nurse/*.jpg')\n",
    "noone_p2 = glob(ROOT_DATA_DIR + 'p2/noone/*.jpg')\n",
    "nurse_p2 = glob(ROOT_DATA_DIR + 'p2/nurse/*.jpg')\n",
    "noone_p5 = glob(ROOT_DATA_DIR + 'p5/noone/*.jpg')\n",
    "nurse_p5 = glob(ROOT_DATA_DIR + 'p5/nurse/*.jpg')\n",
    "noone_p6 = glob(ROOT_DATA_DIR + 'p6/noone/*.jpg')\n",
    "nurse_p6 = glob(ROOT_DATA_DIR + 'p6/nurse/*.jpg')\n",
    "noone_p8 = glob(ROOT_DATA_DIR + 'p8/noone/*.jpg')\n",
    "nurse_p8 = glob(ROOT_DATA_DIR + 'p8/nurse/*.jpg')\n",
    "noone_p9 = glob(ROOT_DATA_DIR + 'p9/noone/*.jpg')\n",
    "nurse_p9 = glob(ROOT_DATA_DIR + 'p9/nurse/*.jpg')\n",
    "noone_p10 = glob(ROOT_DATA_DIR + 'p10/noone/*.jpg')\n",
    "nurse_p10 = glob(ROOT_DATA_DIR + 'p10/nurse/*.jpg')\n",
    "noone_p11 = glob(ROOT_DATA_DIR + 'p11/noone/*.jpg')\n",
    "nurse_p11 = glob(ROOT_DATA_DIR + 'p11/nurse/*.jpg')\n",
    "noone_p13 = glob(ROOT_DATA_DIR + 'p13/noone/*.jpg')\n",
    "nurse_p13 = glob(ROOT_DATA_DIR + 'p13/nurse/*.jpg')\n",
    "noone_p14 = glob(ROOT_DATA_DIR + 'p14/noone/*.jpg')\n",
    "nurse_p14 = glob(ROOT_DATA_DIR + 'p14/nurse/*.jpg')\n",
    "noone_p15 = glob(ROOT_DATA_DIR + 'p15/noone/*.jpg')\n",
    "nurse_p15 = glob(ROOT_DATA_DIR + 'p15/nurse/*.jpg')\n",
    "noone_p16 = glob(ROOT_DATA_DIR + 'p16/noone/*.jpg')\n",
    "nurse_p16 = glob(ROOT_DATA_DIR + 'p16/nurse/*.jpg')\n",
    "noone_p17 = glob(ROOT_DATA_DIR + 'p17/noone/*.jpg')\n",
    "nurse_p17 = glob(ROOT_DATA_DIR + 'p17/nurse/*.jpg')\n",
    "# noone_p18 = glob(ROOT_DATA_DIR + 'p18/noone/*.jpg')\n",
    "# nurse_p18 = glob(ROOT_DATA_DIR + 'p18/nurse/*.jpg')\n",
    "# noone_p19 = glob(ROOT_DATA_DIR + 'p19/noone/*.jpg')\n",
    "# nurse_p19 = glob(ROOT_DATA_DIR + 'p19/nurse/*.jpg')\n",
    "noone_p21 = glob(ROOT_DATA_DIR + 'p21/noone/*.jpg')\n",
    "nurse_p21 = glob(ROOT_DATA_DIR + 'p21/nurse/*.jpg')\n",
    "noone_p22 = glob(ROOT_DATA_DIR + 'p22/noone/*.jpg')\n",
    "nurse_p22 = glob(ROOT_DATA_DIR + 'p22/nurse/*.jpg')\n",
    "noone_p23 = glob(ROOT_DATA_DIR + 'p23/noone/*.jpg')\n",
    "nurse_p23 = glob(ROOT_DATA_DIR + 'p23/nurse/*.jpg')\n",
    "noone_p24 = glob(ROOT_DATA_DIR + 'p24/noone/*.jpg')\n",
    "nurse_p24 = glob(ROOT_DATA_DIR + 'p24/nurse/*.jpg')\n",
    "noone_p25 = glob(ROOT_DATA_DIR + 'p25/noone/*.jpg')\n",
    "nurse_p25 = glob(ROOT_DATA_DIR + 'p25/nurse/*.jpg')\n",
    "noone_p26 = glob(ROOT_DATA_DIR + 'p26/noone/*.jpg')\n",
    "nurse_p26 = glob(ROOT_DATA_DIR + 'p26/nurse/*.jpg')\n",
    "noone_p27 = glob(ROOT_DATA_DIR + 'p27/noone/*.jpg')\n",
    "nurse_p27 = glob(ROOT_DATA_DIR + 'p27/nurse/*.jpg')\n",
    "noone_p28 = glob(ROOT_DATA_DIR + 'p28/noone/*.jpg')\n",
    "nurse_p28 = glob(ROOT_DATA_DIR + 'p28/nurse/*.jpg')\n",
    "noone_p29 = glob(ROOT_DATA_DIR + 'p29/noone/*.jpg')\n",
    "nurse_p29 = glob(ROOT_DATA_DIR + 'p29/nurse/*.jpg')\n",
    "noone_p30 = glob(ROOT_DATA_DIR + 'p30/noone/*.jpg')\n",
    "nurse_p30 = glob(ROOT_DATA_DIR + 'p30/nurse/*.jpg')\n",
    "noone_p31 = glob(ROOT_DATA_DIR + 'p31/noone/*.jpg')\n",
    "nurse_p31 = glob(ROOT_DATA_DIR + 'p31/nurse/*.jpg')\n",
    "noone_p32 = glob(ROOT_DATA_DIR + 'p32/noone/*.jpg')\n",
    "nurse_p32 = glob(ROOT_DATA_DIR + 'p32/nurse/*.jpg')\n",
    "noone_p33 = glob(ROOT_DATA_DIR + 'p33/noone/*.jpg')\n",
    "nurse_p33 = glob(ROOT_DATA_DIR + 'p33/nurse/*.jpg')\n",
    "noone_p34 = glob(ROOT_DATA_DIR + 'p34/noone/*.jpg')\n",
    "nurse_p34 = glob(ROOT_DATA_DIR + 'p34/nurse/*.jpg')\n",
    "# noone_p35 = glob(ROOT_DATA_DIR + 'p35/noone/*.jpg')\n",
    "# nurse_p35 = glob(ROOT_DATA_DIR + 'p35/nurse/*.jpg')\n",
    "# noone_p36 = glob(ROOT_DATA_DIR + 'p36/noone/*.jpg')\n",
    "# nurse_p36 = glob(ROOT_DATA_DIR + 'p36/nurse/*.jpg')\n",
    "# noone_p37 = glob(ROOT_DATA_DIR + 'p37/noone/*.jpg')\n",
    "# nurse_p37 = glob(ROOT_DATA_DIR + 'p37/nurse/*.jpg')\n",
    "# noone_p38 = glob(ROOT_DATA_DIR + 'p38/noone/*.jpg')\n",
    "# nurse_p38 = glob(ROOT_DATA_DIR + 'p38/nurse/*.jpg')\n",
    "\n",
    "# all_noone = noone_p1 + noone_p2 + noone_p5 + noone_p6 + noone_p8 + noone_p9 + noone_p10 + noone_p11 + noone_p13 + noone_p14 + noone_p15 + noone_p16 + noone_p17 + noone_p18 + noone_p19 + noone_p21 + noone_p22 + noone_p23 + noone_p24 + noone_p25 + noone_p26 + noone_p27 + noone_p28 + noone_p29 + noone_p30 + noone_p31 + noone_p32 + noone_p33 + noone_p34 + noone_p35 + noone_p36 + noone_p37 + noone_p38\n",
    "# all_nurse = nurse_p1 + nurse_p2 + nurse_p5 + nurse_p6 + nurse_p8 + nurse_p9 + nurse_p10 + nurse_p11 + nurse_p13 + nurse_p14 + nurse_p15 + nurse_p16 + nurse_p17 + nurse_p18 + nurse_p19 + nurse_p21 + nurse_p22 + nurse_p23 + nurse_p24 + nurse_p25 + nurse_p26 + nurse_p27 + nurse_p28 + nurse_p29 + nurse_p30 + nurse_p31 + nurse_p32 + nurse_p33 + nurse_p34 + nurse_p35 + nurse_p36 + nurse_p37 + nurse_p38\n",
    "\n",
    "# all_noone_list = [noone_p1, noone_p2, noone_p6, noone_p8, noone_p9, noone_p10, noone_p11, noone_p13, noone_p14, noone_p15, noone_p16, noone_p18, noone_p19, noone_p21, noone_p22, noone_p24, noone_p26, noone_p27, noone_p28, noone_p29, noone_p30, noone_p31, noone_p32, noone_p33, noone_p34, noone_p35, noone_p36, noone_p37, noone_p38]\n",
    "# all_nurse_list = [nurse_p1, nurse_p2, nurse_p6, nurse_p8, nurse_p9, nurse_p10, nurse_p11, nurse_p13, nurse_p14, nurse_p15, nurse_p16, nurse_p18, nurse_p19, nurse_p21, nurse_p22, nurse_p24, nurse_p26, nurse_p27, nurse_p28, nurse_p29, nurse_p30, nurse_p31, nurse_p32, nurse_p33, nurse_p34, nurse_p35, nurse_p36, nurse_p37, nurse_p38]\n",
    "\n",
    "# all_noone_list = [noone_p1, noone_p2, noone_p5, noone_p6, noone_p8, noone_p9, noone_p10, noone_p11, noone_p13, noone_p14, noone_p15, noone_p16, noone_p17, noone_p18, noone_p19, noone_p21, noone_p22, noone_p23, noone_p24, noone_p25, noone_p26, noone_p27, noone_p28, noone_p29, noone_p30, noone_p31, noone_p32, noone_p33, noone_p34, noone_p35, noone_p36, noone_p37, noone_p38]\n",
    "# all_nurse_list = [nurse_p1, nurse_p2, nurse_p5, nurse_p6, nurse_p8, nurse_p9, nurse_p10, nurse_p11, nurse_p13, nurse_p14, nurse_p15, nurse_p16, nurse_p17, nurse_p18, nurse_p19, nurse_p21, nurse_p22, nurse_p23, nurse_p24, nurse_p25, nurse_p26, nurse_p27, nurse_p28, nurse_p29, nurse_p30, nurse_p31, nurse_p32, nurse_p33, nurse_p34, nurse_p35, nurse_p36, nurse_p37, nurse_p38]\n",
    "\n",
    "all_noone_list = [noone_p1, noone_p2, noone_p5, noone_p6, noone_p8, noone_p9, noone_p10, noone_p11, noone_p13, noone_p14, noone_p15, noone_p16, noone_p17, noone_p21, noone_p22, noone_p23, noone_p24, noone_p25, noone_p26, noone_p27, noone_p28, noone_p29, noone_p30, noone_p31, noone_p32, noone_p33, noone_p34]\n",
    "all_nurse_list = [nurse_p1, nurse_p2, nurse_p5, nurse_p6, nurse_p8, nurse_p9, nurse_p10, nurse_p11, nurse_p13, nurse_p14, nurse_p15, nurse_p16, nurse_p17, nurse_p21, nurse_p22, nurse_p23, nurse_p24, nurse_p25, nurse_p26, nurse_p27, nurse_p28, nurse_p29, nurse_p30, nurse_p31, nurse_p32, nurse_p33, nurse_p34]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_noone_train = []\n",
    "# all_noone_test = []\n",
    "# for p in all_noone_list:\n",
    "#     if len(p) > 0:\n",
    "#         train, test = train_test_split(p, test_size=TEST_SIZE)\n",
    "#         all_noone_train += [train]\n",
    "#         all_noone_test += test\n",
    "\n",
    "# all_nurse_train = []\n",
    "# all_nurse_test = []\n",
    "# for p in all_nurse_list:\n",
    "#     if len(p) > 0:\n",
    "#         train, test = train_test_split(p, test_size=TEST_SIZE)\n",
    "#         all_nurse_train += [train]\n",
    "#         all_nurse_test += test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_noone_train = []\n",
    "all_noone_test = []\n",
    "all_nurse_train = []\n",
    "all_nurse_test = []\n",
    "\n",
    "if TEST_SIZE > 0:\n",
    "    train, test = train_test_split(range(len(all_noone_list)), test_size=TEST_SIZE)\n",
    "    \n",
    "    for i in train:\n",
    "        all_noone_train += [all_noone_list[i]]\n",
    "        all_nurse_train += [all_nurse_list[i]]\n",
    "\n",
    "    for i in test:\n",
    "        all_noone_test += all_noone_list[i]\n",
    "        all_nurse_test += all_nurse_list[i]\n",
    "elif TEST_SIZE == 0:\n",
    "    all_noone_train = all_noone_list\n",
    "    all_nurse_train = all_nurse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remFiles = glob(TEST_DIR + 'noone/'+ '*.jpg')\n",
    "for f in remFiles:\n",
    "    os.remove(f)\n",
    "\n",
    "remFiles = glob(TEST_DIR + 'nurse/'+ '*.jpg')\n",
    "for f in remFiles:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in all_noone_test:\n",
    "    basename = os.path.basename(f)\n",
    "    dst_path = TEST_DIR + 'noone/' + basename\n",
    "    shutil.copy(f, dst_path)\n",
    "\n",
    "for f in all_nurse_test:\n",
    "    basename = os.path.basename(f)\n",
    "    dst_path = TEST_DIR + 'nurse/' + basename\n",
    "    shutil.copy(f, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noone_test = glob(TEST_DIR + 'noone/*.jpg')\n",
    "nurse_test = glob(TEST_DIR + 'nurse/*.jpg')\n",
    "all_test = (noone_test, nurse_test)\n",
    "labels = ['noone', 'nurse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_batch, y_batch = next(test_generator)\n",
    "\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# plt.style.use('dark_background')\n",
    "# for k, (img, lbl) in enumerate(zip(x_batch, y_batch)):\n",
    "#     plt.subplot(4, 8, k+1)\n",
    "#     plt.imshow((img + 1) / 2)\n",
    "#     plt.title(\"Class: {}\".format(labels[int(lbl)]))\n",
    "#     plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = VGG16(weights='imagenet', include_top=True)\n",
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.layers[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_layers = test_model.layers[2:-1]\n",
    "# trans_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.layers\n",
    "# w = test_model.layers[1].get_weights()[0][:, :, 2, :]\n",
    "# w = w.reshape(3, 3, 1, 64)\n",
    "# b = test_model.layers[1].get_weights()[1]\n",
    "# first_conv2d = Conv2D(64, kernel_size=3, padding='same', kernel_initializer=tf.keras.initializers.zeros(), use_bias=True, activation='relu')\n",
    "\n",
    "# model = tf.keras.Sequential(\n",
    "#     [\n",
    "#         tf.keras.Input(shape=(224, 224, 1),name='input'),\n",
    "#         first_conv2d,\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         tf.keras.layers.Dense(10, activation=\"relu\",use_bias=True,bias_initializer='zeros',name='dense1'),\n",
    "#         tf.keras.layers.Dense(1, activation=\"softmax\",name='output'),\n",
    "#     ]\n",
    "# )\n",
    "# model.layers[0].get_weights()[0].shape\n",
    "\n",
    "# model.layers[0].set_weights([w, b])\n",
    "# model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.layers[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "        # tf.keras.metrics.TruePositives(name='tp'),\n",
    "        # tf.keras.metrics.FalsePositives(name='fp'),\n",
    "        # tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "        # tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "        # tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='AUC'),\n",
    "        tf.keras.metrics.AUC(name='prc', curve='PR'),\n",
    "    ]\n",
    "\n",
    "def create_model(finetuning=True, metrics=METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    \n",
    "    base_model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "    first_conv2d_bias = base_model.layers[1].get_weights()[-1]\n",
    "    first_conv2d_weights = base_model.layers[1].get_weights()[0][:, :, 2, :].reshape(3, 3, 1, 64)\n",
    "    \n",
    "    custom_input = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1), name=\"1-Channel_input\")\n",
    "    first_conv2d = Conv2D(64, kernel_size=3, padding='same', name=\"First_Conv2D\")\n",
    "    transfer_layers = base_model.layers[2:-1]\n",
    "    predictions = Dense(2, activation=\"softmax\", name=\"prediction_layer\", bias_initializer=output_bias)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(custom_input)\n",
    "    model.add(first_conv2d)\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.layers[0].set_weights([first_conv2d_weights, first_conv2d_bias])\n",
    "\n",
    "    for layer in transfer_layers:\n",
    "        # print(layer)\n",
    "        model.add(layer)\n",
    "\n",
    "    model.add(predictions)\n",
    "\n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "        # Use this for \n",
    "        optimizer=tf.optimizers.SGD(0.00001, momentum=0.9),\n",
    "        # optimizer=tf.optimizers.SGD(),\n",
    "        metrics=[METRICS],\n",
    "        )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepFiles(noone_train, nurse_train, noone_val, nurse_val):\n",
    "    # Clean up train and val folders\n",
    "    remFiles = glob(TRAIN_DIR + 'noone/'+ '*.jpg')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(TRAIN_DIR + 'nurse/'+ '*.jpg')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(VAL_DIR + 'noone/'+ '*.jpg')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "    remFiles = glob(VAL_DIR + 'nurse/'+ '*.jpg')\n",
    "    for f in remFiles:\n",
    "        os.remove(f)\n",
    "\n",
    "    # Move files of current fold into their folders\n",
    "    for f in noone_train:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = TRAIN_DIR + 'noone/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in nurse_train:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = TRAIN_DIR + 'nurse/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in noone_val:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = VAL_DIR + 'noone/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "    for f in nurse_val:\n",
    "        basename = os.path.basename(f)\n",
    "        dst_path = VAL_DIR + 'nurse/' + basename\n",
    "        shutil.copy(f, dst_path)\n",
    "\n",
    "    noone_train = glob(TRAIN_DIR + 'noone/*.jpg')\n",
    "    nurse_train = glob(TRAIN_DIR + 'nurse/*.jpg')\n",
    "\n",
    "    noone_val = glob(VAL_DIR + 'noone/*.jpg')\n",
    "    nurse_val = glob(VAL_DIR + 'nurse/*.jpg')\n",
    "\n",
    "    return noone_train, nurse_train, noone_val, nurse_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: \n",
      "\n",
      "noone_train: 10722, nurse_train: 1001\n",
      "noone_val: 2910, nurse_val: 259\n",
      "\n",
      "Class Weights: {0: 0.5462147887323944, 1: 5.90952380952381}\n",
      "\n",
      "Initial Bias: [-2.38130825]\n",
      "\n",
      "Found 11723 images belonging to 2 classes.\n",
      "Found 3169 images belonging to 2 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "367/367 [==============================] - 80s 199ms/step - loss: 0.5964 - accuracy: 0.6776 - precision: 0.6776 - recall: 0.6776 - AUC: 0.7470 - prc: 0.7453\n",
      "Epoch 2/20\n",
      "367/367 [==============================] - 72s 196ms/step - loss: 0.4932 - accuracy: 0.7777 - precision: 0.7777 - recall: 0.7777 - AUC: 0.8512 - prc: 0.8488\n",
      "Epoch 3/20\n",
      "367/367 [==============================] - 72s 196ms/step - loss: 0.4575 - accuracy: 0.7986 - precision: 0.7986 - recall: 0.7986 - AUC: 0.8704 - prc: 0.8654\n",
      "Epoch 4/20\n",
      "367/367 [==============================] - 72s 197ms/step - loss: 0.4225 - accuracy: 0.8209 - precision: 0.8209 - recall: 0.8209 - AUC: 0.8932 - prc: 0.8885\n",
      "Epoch 5/20\n",
      "367/367 [==============================] - 72s 197ms/step - loss: 0.4154 - accuracy: 0.8208 - precision: 0.8208 - recall: 0.8208 - AUC: 0.8941 - prc: 0.8864\n",
      "Epoch 6/20\n",
      "367/367 [==============================] - 72s 197ms/step - loss: 0.3957 - accuracy: 0.8357 - precision: 0.8357 - recall: 0.8357 - AUC: 0.9083 - prc: 0.9047\n",
      "Epoch 7/20\n",
      "367/367 [==============================] - 73s 197ms/step - loss: 0.3901 - accuracy: 0.8392 - precision: 0.8392 - recall: 0.8392 - AUC: 0.9094 - prc: 0.9041\n",
      "Epoch 8/20\n",
      "367/367 [==============================] - 73s 197ms/step - loss: 0.3667 - accuracy: 0.8553 - precision: 0.8553 - recall: 0.8553 - AUC: 0.9236 - prc: 0.9202\n",
      "Epoch 9/20\n",
      "367/367 [==============================] - 72s 196ms/step - loss: 0.3466 - accuracy: 0.8643 - precision: 0.8643 - recall: 0.8643 - AUC: 0.9335 - prc: 0.9314\n",
      "Epoch 10/20\n",
      "367/367 [==============================] - 72s 197ms/step - loss: 0.3359 - accuracy: 0.8687 - precision: 0.8687 - recall: 0.8687 - AUC: 0.9372 - prc: 0.9350\n",
      "Epoch 11/20\n",
      "367/367 [==============================] - 72s 197ms/step - loss: 0.3351 - accuracy: 0.8711 - precision: 0.8711 - recall: 0.8711 - AUC: 0.9388 - prc: 0.9355\n",
      "Epoch 12/20\n",
      "367/367 [==============================] - 72s 197ms/step - loss: 0.3155 - accuracy: 0.8771 - precision: 0.8771 - recall: 0.8771 - AUC: 0.9463 - prc: 0.9441\n",
      "Epoch 13/20\n",
      "367/367 [==============================] - 73s 197ms/step - loss: 0.3083 - accuracy: 0.8835 - precision: 0.8835 - recall: 0.8835 - AUC: 0.9504 - prc: 0.9491\n",
      "Epoch 14/20\n",
      "367/367 [==============================] - 73s 197ms/step - loss: 0.3033 - accuracy: 0.8830 - precision: 0.8830 - recall: 0.8830 - AUC: 0.9496 - prc: 0.9477\n",
      "Epoch 15/20\n",
      "367/367 [==============================] - 72s 197ms/step - loss: 0.2845 - accuracy: 0.8951 - precision: 0.8951 - recall: 0.8951 - AUC: 0.9596 - prc: 0.9586\n",
      "Epoch 16/20\n",
      "367/367 [==============================] - 72s 197ms/step - loss: 0.2796 - accuracy: 0.8979 - precision: 0.8979 - recall: 0.8979 - AUC: 0.9610 - prc: 0.9598\n",
      "Epoch 17/20\n",
      "367/367 [==============================] - 71s 194ms/step - loss: 0.2792 - accuracy: 0.8995 - precision: 0.8995 - recall: 0.8995 - AUC: 0.9602 - prc: 0.9592\n",
      "Epoch 18/20\n",
      "367/367 [==============================] - 70s 191ms/step - loss: 0.2628 - accuracy: 0.9103 - precision: 0.9103 - recall: 0.9103 - AUC: 0.9658 - prc: 0.9646\n",
      "Epoch 19/20\n",
      "367/367 [==============================] - 70s 191ms/step - loss: 0.2632 - accuracy: 0.9086 - precision: 0.9086 - recall: 0.9086 - AUC: 0.9650 - prc: 0.9637\n",
      "Epoch 20/20\n",
      "367/367 [==============================] - 70s 191ms/step - loss: 0.2616 - accuracy: 0.9082 - precision: 0.9082 - recall: 0.9082 - AUC: 0.9657 - prc: 0.9644\n",
      "Fold 1: \n",
      "\n",
      "noone_train: 10416, nurse_train: 878\n",
      "noone_val: 3216, nurse_val: 382\n",
      "\n",
      "Class Weights: {0: 0.5462147887323944, 1: 5.90952380952381}\n",
      "\n",
      "Initial Bias: [-2.38130825]\n",
      "\n",
      "Found 11294 images belonging to 2 classes.\n",
      "Found 3598 images belonging to 2 classes.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "353/353 [==============================] - 70s 196ms/step - loss: 0.5307 - accuracy: 0.8187 - precision: 0.8187 - recall: 0.8187 - AUC: 0.9143 - prc: 0.9159\n",
      "Epoch 2/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.4350 - accuracy: 0.7993 - precision: 0.7993 - recall: 0.7993 - AUC: 0.8908 - prc: 0.8935\n",
      "Epoch 3/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.3941 - accuracy: 0.8361 - precision: 0.8361 - recall: 0.8361 - AUC: 0.9159 - prc: 0.9156\n",
      "Epoch 4/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.3691 - accuracy: 0.8543 - precision: 0.8543 - recall: 0.8543 - AUC: 0.9276 - prc: 0.9269\n",
      "Epoch 5/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.3480 - accuracy: 0.8704 - precision: 0.8704 - recall: 0.8704 - AUC: 0.9373 - prc: 0.9351\n",
      "Epoch 6/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.3412 - accuracy: 0.8726 - precision: 0.8726 - recall: 0.8726 - AUC: 0.9402 - prc: 0.9395\n",
      "Epoch 7/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.3369 - accuracy: 0.8763 - precision: 0.8763 - recall: 0.8763 - AUC: 0.9418 - prc: 0.9396\n",
      "Epoch 8/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.3236 - accuracy: 0.8830 - precision: 0.8830 - recall: 0.8830 - AUC: 0.9462 - prc: 0.9440\n",
      "Epoch 9/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.3196 - accuracy: 0.8830 - precision: 0.8830 - recall: 0.8830 - AUC: 0.9473 - prc: 0.9459\n",
      "Epoch 10/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.2967 - accuracy: 0.8932 - precision: 0.8932 - recall: 0.8932 - AUC: 0.9568 - prc: 0.9552\n",
      "Epoch 11/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.2976 - accuracy: 0.8952 - precision: 0.8952 - recall: 0.8952 - AUC: 0.9562 - prc: 0.9540\n",
      "Epoch 12/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.2843 - accuracy: 0.9065 - precision: 0.9065 - recall: 0.9065 - AUC: 0.9611 - prc: 0.9588\n",
      "Epoch 13/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.2876 - accuracy: 0.9046 - precision: 0.9046 - recall: 0.9046 - AUC: 0.9606 - prc: 0.9586\n",
      "Epoch 14/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.2791 - accuracy: 0.9034 - precision: 0.9034 - recall: 0.9034 - AUC: 0.9605 - prc: 0.9584\n",
      "Epoch 15/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.2652 - accuracy: 0.9154 - precision: 0.9154 - recall: 0.9154 - AUC: 0.9677 - prc: 0.9650\n",
      "Epoch 16/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.2601 - accuracy: 0.9200 - precision: 0.9200 - recall: 0.9200 - AUC: 0.9678 - prc: 0.9652\n",
      "Epoch 17/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.2488 - accuracy: 0.9251 - precision: 0.9251 - recall: 0.9251 - AUC: 0.9719 - prc: 0.9697\n",
      "Epoch 18/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.2488 - accuracy: 0.9251 - precision: 0.9251 - recall: 0.9251 - AUC: 0.9717 - prc: 0.9694\n",
      "Epoch 19/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.2513 - accuracy: 0.9177 - precision: 0.9177 - recall: 0.9177 - AUC: 0.9694 - prc: 0.9672\n",
      "Epoch 20/20\n",
      "353/353 [==============================] - 68s 191ms/step - loss: 0.2335 - accuracy: 0.9287 - precision: 0.9287 - recall: 0.9287 - AUC: 0.9750 - prc: 0.9731\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=K_FOLDS)\n",
    "cv_split = kf.split(range(len(all_noone_train)))\n",
    "foldNum = 0\n",
    "\n",
    "foldHistories = []\n",
    "# foldMetrics = []\n",
    "\n",
    "# with tf.device('/CPU:0'):\n",
    "for train_index, val_index in cv_split:\n",
    "    if foldNum > 1:\n",
    "        foldNum += 1\n",
    "        continue\n",
    "    print(\"Fold {}: \\n\".format(foldNum))\n",
    "\n",
    "    # Select the current folds for training and validation\n",
    "    noone_train = []\n",
    "    nurse_train = []\n",
    "    noone_val = []\n",
    "    nurse_val = []\n",
    "    for i in train_index:\n",
    "        noone_train += all_noone_train[i]\n",
    "        nurse_train += all_nurse_train[i]\n",
    "    for i in val_index:\n",
    "        noone_val += all_noone_train[i]\n",
    "        nurse_val += all_nurse_train[i]\n",
    "\n",
    "    # Move images to training and testing folders\n",
    "    noone_train, nurse_train, noone_val, nurse_val = prepFiles(noone_train, nurse_train, noone_val, nurse_val)\n",
    "    all_train = [noone_train, nurse_train]\n",
    "    all_val = [noone_val, nurse_val]\n",
    "\n",
    "    print(\"noone_train: {}, nurse_train: {}\".format(len(noone_train), len(nurse_train)))\n",
    "    print(\"noone_val: {}, nurse_val: {}\\n\".format(len(noone_val), len(nurse_val)))\n",
    "\n",
    "    weight_for_0 = (1 / (len(noone_train) + len(noone_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "    weight_for_1 = (1 / (len(nurse_train) + len(nurse_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "    CLASS_WEIGHT = {0: weight_for_0, 1: weight_for_1}\n",
    "    print(\"Class Weights: {}\\n\".format(CLASS_WEIGHT))\n",
    "    \n",
    "    INITIAL_BIAS = np.log([(len(nurse_train) + len(nurse_val)) / (len(noone_train) + len(noone_val))])\n",
    "    print(\"Initial Bias: {}\\n\".format(INITIAL_BIAS))\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=123,\n",
    "    )\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        VAL_DIR,\n",
    "        target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=123,\n",
    "    )\n",
    "\n",
    "    # model = create_model(output_bias=INITIAL_BIAS)\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    checkpoint_path = \"SavedModels/cp_fold_{}_class_weight.ckpt\".format(foldNum)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_prc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "    earlyStopping = EarlyStopping(monitor='prc', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "\n",
    "    history = model.fit(\n",
    "        x=train_generator,\n",
    "        epochs=20,\n",
    "        # validation_data=val_generator,\n",
    "        # validation_steps=1,\n",
    "        # callbacks=[checkpoint],\n",
    "        # callbacks=[checkpoint, earlyStopping],\n",
    "        class_weight=CLASS_WEIGHT,\n",
    "    )\n",
    "\n",
    "    model.save_weights(checkpoint_path)\n",
    "\n",
    "    foldHistories.append(history)\n",
    "\n",
    "    # metrics = model.evaluate(test_generator, return_dict=True)\n",
    "    # for name, value in metrics.items():\n",
    "    #     print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "    # foldMetrics.append(metrics)\n",
    "\n",
    "    foldNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Zalamaan\\Documents\\Repos\\depthSliceTool\\InterventionDetection_origData.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X32sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     nurse_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m all_nurse_train[i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X32sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Move images to training and testing folders\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X32sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m noone_train, nurse_train, noone_val, nurse_val \u001b[39m=\u001b[39m prepFiles(noone_train, nurse_train, noone_val, nurse_val)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X32sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m all_train \u001b[39m=\u001b[39m [noone_train, nurse_train]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X32sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m all_val \u001b[39m=\u001b[39m [noone_val, nurse_val]\n",
      "\u001b[1;32mc:\\Users\\Zalamaan\\Documents\\Repos\\depthSliceTool\\InterventionDetection_origData.ipynb Cell 24\u001b[0m in \u001b[0;36mprepFiles\u001b[1;34m(noone_train, nurse_train, noone_val, nurse_val)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X32sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     basename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(f)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X32sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     dst_path \u001b[39m=\u001b[39m TRAIN_DIR \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnoone/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m basename\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopy(f, dst_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X32sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m nurse_train:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     basename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(f)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\shutil.py:419\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    417\u001b[0m     dst \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dst, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(src))\n\u001b[0;32m    418\u001b[0m copyfile(src, dst, follow_symlinks\u001b[39m=\u001b[39mfollow_symlinks)\n\u001b[1;32m--> 419\u001b[0m copymode(src, dst, follow_symlinks\u001b[39m=\u001b[39;49mfollow_symlinks)\n\u001b[0;32m    420\u001b[0m \u001b[39mreturn\u001b[39;00m dst\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\shutil.py:308\u001b[0m, in \u001b[0;36mcopymode\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    305\u001b[0m     stat_func, chmod_func \u001b[39m=\u001b[39m _stat, os\u001b[39m.\u001b[39mchmod\n\u001b[0;32m    307\u001b[0m st \u001b[39m=\u001b[39m stat_func(src)\n\u001b[1;32m--> 308\u001b[0m chmod_func(dst, stat\u001b[39m.\u001b[39;49mS_IMODE(st\u001b[39m.\u001b[39;49mst_mode))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DELETE THIS\n",
    "\n",
    "kf = KFold(n_splits=K_FOLDS)\n",
    "cv_split = kf.split(range(len(all_noone_train)))\n",
    "foldNum = 0\n",
    "\n",
    "foldHistories = []\n",
    "# foldMetrics = []\n",
    "\n",
    "# with tf.device('/CPU:0'):\n",
    "for train_index, val_index in cv_split:\n",
    "    if foldNum < 2:\n",
    "        foldNum += 1\n",
    "        continue\n",
    "    elif foldNum > 3:\n",
    "        foldNum += 1\n",
    "        continue\n",
    "    print(\"Fold {}: \\n\".format(foldNum))\n",
    "\n",
    "    # Select the current folds for training and validation\n",
    "    noone_train = []\n",
    "    nurse_train = []\n",
    "    noone_val = []\n",
    "    nurse_val = []\n",
    "    for i in train_index:\n",
    "        noone_train += all_noone_train[i]\n",
    "        nurse_train += all_nurse_train[i]\n",
    "    for i in val_index:\n",
    "        noone_val += all_noone_train[i]\n",
    "        nurse_val += all_nurse_train[i]\n",
    "\n",
    "    # Move images to training and testing folders\n",
    "    noone_train, nurse_train, noone_val, nurse_val = prepFiles(noone_train, nurse_train, noone_val, nurse_val)\n",
    "    all_train = [noone_train, nurse_train]\n",
    "    all_val = [noone_val, nurse_val]\n",
    "\n",
    "    print(\"noone_train: {}, nurse_train: {}\".format(len(noone_train), len(nurse_train)))\n",
    "    print(\"noone_val: {}, nurse_val: {}\\n\".format(len(noone_val), len(nurse_val)))\n",
    "\n",
    "    weight_for_0 = (1 / (len(noone_train) + len(noone_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "    weight_for_1 = (1 / (len(nurse_train) + len(nurse_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "    CLASS_WEIGHT = {0: weight_for_0, 1: weight_for_1}\n",
    "    print(\"Class Weights: {}\\n\".format(CLASS_WEIGHT))\n",
    "    \n",
    "    INITIAL_BIAS = np.log([(len(nurse_train) + len(nurse_val)) / (len(noone_train) + len(noone_val))])\n",
    "    print(\"Initial Bias: {}\\n\".format(INITIAL_BIAS))\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=123,\n",
    "    )\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        VAL_DIR,\n",
    "        target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=123,\n",
    "    )\n",
    "\n",
    "    # model = create_model(output_bias=INITIAL_BIAS)\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    checkpoint_path = \"SavedModels/cp_fold_{}_class_weight.ckpt\".format(foldNum)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_prc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "    earlyStopping = EarlyStopping(monitor='prc', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "\n",
    "    history = model.fit(\n",
    "        x=train_generator,\n",
    "        epochs=20,\n",
    "        # validation_data=val_generator,\n",
    "        # validation_steps=1,\n",
    "        # callbacks=[checkpoint],\n",
    "        # callbacks=[checkpoint, earlyStopping],\n",
    "        class_weight=CLASS_WEIGHT,\n",
    "    )\n",
    "\n",
    "    model.save_weights(checkpoint_path)\n",
    "\n",
    "    foldHistories.append(history)\n",
    "\n",
    "    # metrics = model.evaluate(test_generator, return_dict=True)\n",
    "    # for name, value in metrics.items():\n",
    "    #     print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "    # foldMetrics.append(metrics)\n",
    "\n",
    "    foldNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: \n",
      "\n",
      "noone_train: 11250, nurse_train: 1092\n",
      "noone_val: 2382, nurse_val: 168\n",
      "\n",
      "Class Weights: {0: 0.5462147887323944, 1: 5.90952380952381}\n",
      "\n",
      "Initial Bias: [-2.38130825]\n",
      "\n",
      "Found 12342 images belonging to 2 classes.\n",
      "Found 2550 images belonging to 2 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " First_Conv2D (Conv2D)       (None, 224, 224, 64)      640       \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " prediction_layer (Dense)    (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,267,586\n",
      "Trainable params: 134,267,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "386/386 [==============================] - 85s 204ms/step - loss: 0.2996 - accuracy: 0.9039 - precision: 0.9039 - recall: 0.9039 - AUC: 0.9339 - prc: 0.9174\n",
      "Epoch 2/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.2593 - accuracy: 0.9115 - precision: 0.9115 - recall: 0.9115 - AUC: 0.9634 - prc: 0.9622\n",
      "Epoch 3/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.2276 - accuracy: 0.9128 - precision: 0.9128 - recall: 0.9128 - AUC: 0.9719 - prc: 0.9713\n",
      "Epoch 4/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.2062 - accuracy: 0.9255 - precision: 0.9255 - recall: 0.9255 - AUC: 0.9740 - prc: 0.9727\n",
      "Epoch 5/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.1917 - accuracy: 0.9296 - precision: 0.9296 - recall: 0.9296 - AUC: 0.9770 - prc: 0.9758\n",
      "Epoch 6/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.1851 - accuracy: 0.9328 - precision: 0.9328 - recall: 0.9328 - AUC: 0.9782 - prc: 0.9772\n",
      "Epoch 7/20\n",
      "386/386 [==============================] - 78s 203ms/step - loss: 0.1793 - accuracy: 0.9345 - precision: 0.9345 - recall: 0.9345 - AUC: 0.9793 - prc: 0.9782\n",
      "Epoch 8/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.1749 - accuracy: 0.9373 - precision: 0.9373 - recall: 0.9373 - AUC: 0.9801 - prc: 0.9791\n",
      "Epoch 9/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.1718 - accuracy: 0.9369 - precision: 0.9369 - recall: 0.9369 - AUC: 0.9808 - prc: 0.9799\n",
      "Epoch 10/20\n",
      "386/386 [==============================] - 78s 201ms/step - loss: 0.1689 - accuracy: 0.9382 - precision: 0.9382 - recall: 0.9382 - AUC: 0.9814 - prc: 0.9806\n",
      "Epoch 11/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.1634 - accuracy: 0.9413 - precision: 0.9413 - recall: 0.9413 - AUC: 0.9824 - prc: 0.9815\n",
      "Epoch 12/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.1601 - accuracy: 0.9432 - precision: 0.9432 - recall: 0.9432 - AUC: 0.9830 - prc: 0.9824\n",
      "Epoch 13/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.1558 - accuracy: 0.9447 - precision: 0.9447 - recall: 0.9447 - AUC: 0.9839 - prc: 0.9831\n",
      "Epoch 14/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.1536 - accuracy: 0.9476 - precision: 0.9476 - recall: 0.9476 - AUC: 0.9841 - prc: 0.9833\n",
      "Epoch 15/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.1499 - accuracy: 0.9477 - precision: 0.9477 - recall: 0.9477 - AUC: 0.9848 - prc: 0.9840\n",
      "Epoch 16/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.1478 - accuracy: 0.9471 - precision: 0.9471 - recall: 0.9471 - AUC: 0.9851 - prc: 0.9843\n",
      "Epoch 17/20\n",
      "386/386 [==============================] - 78s 202ms/step - loss: 0.1466 - accuracy: 0.9498 - precision: 0.9498 - recall: 0.9498 - AUC: 0.9853 - prc: 0.9848\n",
      "Epoch 18/20\n",
      "386/386 [==============================] - 75s 193ms/step - loss: 0.1411 - accuracy: 0.9512 - precision: 0.9512 - recall: 0.9512 - AUC: 0.9864 - prc: 0.9857\n",
      "Epoch 19/20\n",
      "386/386 [==============================] - 73s 188ms/step - loss: 0.1393 - accuracy: 0.9515 - precision: 0.9515 - recall: 0.9515 - AUC: 0.9865 - prc: 0.9858\n",
      "Epoch 20/20\n",
      "386/386 [==============================] - 73s 188ms/step - loss: 0.1360 - accuracy: 0.9526 - precision: 0.9526 - recall: 0.9526 - AUC: 0.9871 - prc: 0.9861\n"
     ]
    }
   ],
   "source": [
    "# AND THIS\n",
    "\n",
    "kf = KFold(n_splits=K_FOLDS)\n",
    "cv_split = kf.split(range(len(all_noone_train)))\n",
    "foldNum = 0\n",
    "\n",
    "foldHistories = []\n",
    "# foldMetrics = []\n",
    "\n",
    "# with tf.device('/CPU:0'):\n",
    "for train_index, val_index in cv_split:\n",
    "    if foldNum < 4:\n",
    "        foldNum += 1\n",
    "        continue\n",
    "    print(\"Fold {}: \\n\".format(foldNum))\n",
    "\n",
    "    # Select the current folds for training and validation\n",
    "    noone_train = []\n",
    "    nurse_train = []\n",
    "    noone_val = []\n",
    "    nurse_val = []\n",
    "    for i in train_index:\n",
    "        noone_train += all_noone_train[i]\n",
    "        nurse_train += all_nurse_train[i]\n",
    "    for i in val_index:\n",
    "        noone_val += all_noone_train[i]\n",
    "        nurse_val += all_nurse_train[i]\n",
    "\n",
    "    # Move images to training and testing folders\n",
    "    noone_train, nurse_train, noone_val, nurse_val = prepFiles(noone_train, nurse_train, noone_val, nurse_val)\n",
    "    all_train = [noone_train, nurse_train]\n",
    "    all_val = [noone_val, nurse_val]\n",
    "\n",
    "    print(\"noone_train: {}, nurse_train: {}\".format(len(noone_train), len(nurse_train)))\n",
    "    print(\"noone_val: {}, nurse_val: {}\\n\".format(len(noone_val), len(nurse_val)))\n",
    "\n",
    "    weight_for_0 = (1 / (len(noone_train) + len(noone_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "    weight_for_1 = (1 / (len(nurse_train) + len(nurse_val))) * ((len(nurse_train) + len(nurse_val) + len(noone_train) + len(noone_val)) / 2.0)\n",
    "    CLASS_WEIGHT = {0: weight_for_0, 1: weight_for_1}\n",
    "    print(\"Class Weights: {}\\n\".format(CLASS_WEIGHT))\n",
    "    \n",
    "    INITIAL_BIAS = np.log([(len(nurse_train) + len(nurse_val)) / (len(noone_train) + len(noone_val))])\n",
    "    print(\"Initial Bias: {}\\n\".format(INITIAL_BIAS))\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=123,\n",
    "    )\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        VAL_DIR,\n",
    "        target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=123,\n",
    "    )\n",
    "\n",
    "    # model = create_model(output_bias=INITIAL_BIAS)\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    checkpoint_path = \"SavedModels/cp_fold_{}_class_weight.ckpt\".format(foldNum)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_prc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "    earlyStopping = EarlyStopping(monitor='prc', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "\n",
    "    history = model.fit(\n",
    "        x=train_generator,\n",
    "        epochs=20,\n",
    "        # validation_data=val_generator,\n",
    "        # validation_steps=1,\n",
    "        # callbacks=[checkpoint],\n",
    "        # callbacks=[checkpoint, earlyStopping],\n",
    "        class_weight=CLASS_WEIGHT,\n",
    "    )\n",
    "\n",
    "    model.save_weights(checkpoint_path)\n",
    "\n",
    "    foldHistories.append(history)\n",
    "\n",
    "    # metrics = model.evaluate(test_generator, return_dict=True)\n",
    "    # for name, value in metrics.items():\n",
    "    #     print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "    # foldMetrics.append(metrics)\n",
    "\n",
    "    foldNum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set includes 0 negatives and 0 positives\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noone_test_final = glob(TEST_DIR + 'noone/*.jpg')\n",
    "nurse_test_final = glob(TEST_DIR + 'nurse/*.jpg')\n",
    "print(\"Test set includes {} negatives and {} positives\".format(len(noone_test_final), len(nurse_test_final)))\n",
    "\n",
    "def printCM(cm, labels):\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax=ax)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticklabels(labels)\n",
    "    return\n",
    "\n",
    "def predictNurse(model, img):\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    preds = model.predict(x, verbose=0)\n",
    "    label = 'nurse' if preds[0][1]>preds[0][0] else 'noone'\n",
    "    return label\n",
    "\n",
    "def evalModel(model):\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for i in range(len(noone_test_final)):\n",
    "        im = load_img(noone_test_final[i], color_mode='grayscale', target_size=(IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        label = predictNurse(model, im)\n",
    "        preds.append(label)\n",
    "        truths.append('noone')\n",
    "\n",
    "    for i in range(len(nurse_test_final)):\n",
    "        im = load_img(nurse_test_final[i], color_mode='grayscale', target_size=(IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        label = predictNurse(model, im)\n",
    "        preds.append(label)\n",
    "        truths.append('nurse')\n",
    "\n",
    "    labels = ['noone', 'nurse']\n",
    "    cm = confusion_matrix(truths, preds)\n",
    "    printCM(cm, labels)\n",
    "\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TP = cm[1][1]\n",
    "    prec = TP / (TP + FP)\n",
    "    spec = TN / (TN + FP)\n",
    "    sens = TP / (TP + FN)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    f1 = (2 * TP) / ((2 * TP) + FP + FN)\n",
    "    print(\"Sens: {}, Spec: {}, Prec: {}, Acc: {}, F1: {}\".format(sens, spec, prec, acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_split_testing = kf.split(range(len(all_noone_train)))\n",
    "# for train_index, val_index in cv_split_testing:\n",
    "#     for i in val_index:\n",
    "#         print(all_nurse_list[i][0])\n",
    "#     print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "noone_train: 10722, nurse_train: 1001\n",
      "noone_val: 2910, nurse_val: 259\n",
      "\n",
      "Sens: 0.9768339768339769, Spec: 0.2824742268041237, Prec: 0.10807347287483982, Acc: 0.33922372988324395, F1: 0.1946153846153846\n",
      "Fold 1:\n",
      "noone_train: 10416, nurse_train: 878\n",
      "noone_val: 3216, nurse_val: 382\n",
      "\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Zalamaan\\Documents\\Repos\\depthSliceTool\\InterventionDetection_origData.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m model \u001b[39m=\u001b[39m create_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m model\u001b[39m.\u001b[39mload_weights(\u001b[39m\"\u001b[39m\u001b[39mSavedModels/cp_fold_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_class_weight.ckpt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(foldNum_testing))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m evalModel(model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m foldNum_testing \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\Zalamaan\\Documents\\Repos\\depthSliceTool\\InterventionDetection_origData.ipynb Cell 29\u001b[0m in \u001b[0;36mevalModel\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(noone_test_final)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     im \u001b[39m=\u001b[39m load_img(noone_test_final[i], color_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgrayscale\u001b[39m\u001b[39m'\u001b[39m, target_size\u001b[39m=\u001b[39m(IMAGE_HEIGHT, IMAGE_WIDTH))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     label \u001b[39m=\u001b[39m predictNurse(model, im)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     preds\u001b[39m.\u001b[39mappend(label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     truths\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mnoone\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Zalamaan\\Documents\\Repos\\depthSliceTool\\InterventionDetection_origData.ipynb Cell 29\u001b[0m in \u001b[0;36mpredictNurse\u001b[1;34m(model, img)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x \u001b[39m=\u001b[39m img_to_array(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(x, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(x, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnurse\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m preds[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m>\u001b[39mpreds[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnoone\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zalamaan/Documents/Repos/depthSliceTool/InterventionDetection_origData.ipynb#X40sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m label\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\keras\\engine\\training.py:2033\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2031\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2032\u001b[0m   callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2033\u001b[0m   tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[0;32m   2034\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2035\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Zalamaan\\miniconda3\\envs\\DST\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kf = KFold(n_splits=K_FOLDS)\n",
    "cv_split_testing = kf.split(range(len(all_noone_train)))\n",
    "foldNum_testing = 0\n",
    "\n",
    "# with tf.device('/CPU:0'):\n",
    "for train_index, val_index in cv_split_testing:\n",
    "    print(\"Fold {}:\".format(foldNum_testing))\n",
    "\n",
    "    # Select the current folds for training and validation\n",
    "    noone_train = []\n",
    "    nurse_train = []\n",
    "    noone_val = []\n",
    "    nurse_val = []\n",
    "    for i in train_index:\n",
    "        noone_train += all_noone_train[i]\n",
    "        nurse_train += all_nurse_train[i]\n",
    "    for i in val_index:\n",
    "        noone_val += all_noone_train[i]\n",
    "        nurse_val += all_nurse_train[i]\n",
    "\n",
    "    # Move images to training and testing folders\n",
    "    noone_train, nurse_train, noone_val, nurse_val = prepFiles(noone_train, nurse_train, noone_val, nurse_val)\n",
    "    all_train = [noone_train, nurse_train]\n",
    "    all_val = [noone_val, nurse_val]\n",
    "\n",
    "    noone_test_final = glob(VAL_DIR + 'noone/*.jpg')\n",
    "    nurse_test_final = glob(VAL_DIR + 'nurse/*.jpg')\n",
    "\n",
    "\n",
    "    print(\"noone_train: {}, nurse_train: {}\".format(len(noone_train), len(nurse_train)))\n",
    "    print(\"noone_val: {}, nurse_val: {}\\n\".format(len(noone_val), len(nurse_val)))\n",
    "\n",
    "    model = create_model()\n",
    "    model.load_weights(\"SavedModels/cp_fold_{}_class_weight.ckpt\".format(foldNum_testing))\n",
    "    \n",
    "    evalModel(model)\n",
    "    \n",
    "\n",
    "    foldNum_testing += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf8b9ac8caee8cd6ae392661a519e2d4fbbc720f8ec71fc2b548fd3ad16b33c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
